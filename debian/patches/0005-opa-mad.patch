Description: Add jumbo MAD support to ib_mad.
 OPA requires jumbo MAD (OPA MAD) support. This patch adds the header definitions and
 patches ib_mad to add this support. This also patches hfi1's interface to the mad agent,
 so it complies with the ABI for kernel 3.16.
 .
 ifs-kernel-updates (3.16.0-731-1ifs) UNRELEASED; urgency=medium
 .
     * Import ifs-kernel-updates 3.10.0_327 731 from IFS 10.5.0.0.155 upstream.
Author: Brian T. Smith <bsmith@systemfabricworks.com>
Copyright: 2017 System Fabric Works, Inc. All Rights Reserved.
Forwarded: not-needed
Last-Update: <2017-09-11>

---

--- a/include/rdma/ib_mad.h
+++ b/include/rdma/ib_mad.h
@@ -40,9 +40,16 @@
 #include <linux/list.h>
 
 #include <rdma/ib_verbs.h>
+#include "update/ib_user_mad.h"
+
+/* FIXME for upstream add to ib_verbs.h ASAP */
+enum {
+	IB_DEVICE_OPA_MAD_SUPPORT = (1<<31)
+};
 
 /* Management base version */
 #define IB_MGMT_BASE_VERSION			1
+#define OPA_MGMT_BASE_VERSION			0x80
 
 /* Management classes */
 #define IB_MGMT_CLASS_SUBN_LID_ROUTED		0x01
@@ -134,6 +141,12 @@
 	IB_MGMT_SA_DATA = 200,
 	IB_MGMT_DEVICE_HDR = 64,
 	IB_MGMT_DEVICE_DATA = 192,
+
+	OPA_MGMT_MAD_HDR = IB_MGMT_MAD_HDR,
+	OPA_MGMT_MAD_DATA = 2024,
+	OPA_MGMT_RMPP_HDR = IB_MGMT_RMPP_HDR,
+	OPA_MGMT_RMPP_DATA = 2012,
+	OPA_MGMT_MAD_SIZE = IB_MGMT_MAD_HDR + OPA_MGMT_MAD_DATA,
 };
 
 struct ib_mad_hdr {
@@ -180,12 +193,26 @@
 	u8			data[IB_MGMT_MAD_DATA];
 };
 
-struct ib_rmpp_mad {
+struct opa_mad {
+	struct ib_mad_hdr	mad_hdr;
+	u8			data[OPA_MGMT_MAD_DATA];
+};
+
+struct ib_rmpp_base {
 	struct ib_mad_hdr	mad_hdr;
 	struct ib_rmpp_hdr	rmpp_hdr;
+} __attribute__ ((packed));
+
+struct ib_rmpp_mad {
+	struct ib_rmpp_base	base;
 	u8			data[IB_MGMT_RMPP_DATA];
 };
 
+struct opa_rmpp_mad {
+	struct ib_rmpp_base	base;
+	u8			data[OPA_MGMT_RMPP_DATA];
+};
+
 struct ib_sa_mad {
 	struct ib_mad_hdr	mad_hdr;
 	struct ib_rmpp_hdr	rmpp_hdr;
@@ -247,6 +274,9 @@
 struct ib_mad_send_buf {
 	struct ib_mad_send_buf	*next;
 	void			*mad;
+			/* can be ib_mad or opa_mad
+			 * Depending on base_version in header
+			 */
 	struct ib_mad_agent	*mad_agent;
 	struct ib_ah		*ah;
 	void			*context[2];
@@ -355,9 +385,18 @@
  * @hi_tid: Access layer assigned transaction ID for this client.
  *   Unsolicited MADs sent by this client will have the upper 32-bits
  *   of their TID set to this value.
+ * @flags: registration flags
  * @port_num: Port number on which QP is registered
  * @rmpp_version: If set, indicates the RMPP version used by this agent.
  */
+enum {
+	IB_MAD_USER_RMPP = IB_USER_MAD_USER_RMPP,
+};
+
+#define RMPP_VERSION(x) (x & 0x0f)
+#define RMPP_FLAGS(x) ((x & 0xf0) >> 4)
+#define SET_FLAGS_RMPP(ver, flags) (ver | ((flags & 0x0f) << 4))
+
 struct ib_mad_agent {
 	struct ib_device	*device;
 	struct ib_qp		*qp;
@@ -367,6 +406,7 @@
 	ib_mad_snoop_handler	snoop_handler;
 	void			*context;
 	u32			hi_tid;
+	u32			flags;
 	u8			port_num;
 	u8			rmpp_version;
 };
@@ -395,7 +435,8 @@
 struct ib_mad_recv_buf {
 	struct list_head	list;
 	struct ib_grh		*grh;
-	struct ib_mad		*mad;
+	struct ib_mad		*mad; /* Note can be a struct opa_mad
+					mad_len */
 };
 
 /**
@@ -408,6 +449,17 @@
  * For received response, the wr_id contains a pointer to the ib_mad_send_buf
  *   for the corresponding send request.
  */
+/* FIXME WARNING for upstream.
+ * recv_buf is not a continugous buffer but rather a list of the buffers which
+ * came in for the rmpp response (or a single buffer for non-rmpp).  I _think_
+ * this is fine if these buffers are opa sized as the old clients should not
+ * use the extra memory but I want to verify this.
+ *
+ * Regardless mad_len == num_segments * seg_size
+ *      where seg_size == IB or Opa mad size
+ *      depending on base_version
+ */
+/* FIXME when mad_recv_wc is opa it is not necessarily 2048 bytes */
 struct ib_mad_recv_wc {
 	struct ib_wc		*wc;
 	struct ib_mad_recv_buf	recv_buf;
@@ -426,6 +478,7 @@
  *   in the range from 0x30 to 0x4f. Otherwise not used.
  * @method_mask: The caller will receive unsolicited MADs for any method
  *   where @method_mask = 1.
+ *
  */
 struct ib_mad_reg_req {
 	u8	mgmt_class;
@@ -451,6 +504,7 @@
  * @recv_handler: The completion callback routine invoked for a received
  *   MAD.
  * @context: User specified context associated with the registration.
+ * @registration_flags: Registration flags to set for this agent
  */
 struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 					   u8 port_num,
@@ -661,4 +715,11 @@
  */
 void ib_free_send_mad(struct ib_mad_send_buf *send_buf);
 
+/**
+ * ib_mad_kernel_rmpp_agent - Returns if the agent is performing RMPP.
+ * @agent: the agent in question
+ * @return: true if agent is performing rmpp, false otherwise.
+ */
+int ib_mad_kernel_rmpp_agent(struct ib_mad_agent *agent);
+
 #endif /* IB_MAD_H */
--- a/include/update/ib_user_mad.h
+++ b/include/update/ib_user_mad.h
@@ -191,6 +191,45 @@
 	__u8	rmpp_version;
 };
 
+/**
+ * ib_user_mad_reg_req2 - MAD registration request
+ *
+ * @id                 - Set by the _kernel_; used by userspace to identify the
+ *                       registered agent in future requests.
+ * @qpn                - Queue pair number; must be 0 or 1.
+ * @mgmt_class         - Indicates which management class of MADs should be
+ *                       receive by the caller.  This field is only required if
+ *                       the user wishes to receive unsolicited MADs, otherwise
+ *                       it should be 0.
+ * @mgmt_class_version - Indicates which version of MADs for the given
+ *                       management class to receive.
+ * @res                - Ignored.
+ * @flags              - additional registration flags; Must be in the set of
+ *                       flags defined in IB_USER_MAD_REG_FLAGS_CAP
+ * @method_mask        - The caller wishes to receive unsolicited MADs for the
+ *                       methods whose bit(s) is(are) set.
+ * @oui                - Indicates IEEE OUI to use when mgmt_class is a vendor
+ *                       class in the range from 0x30 to 0x4f. Otherwise not
+ *                       used.
+ * @rmpp_version       - If set, indicates the RMPP version to use.
+ */
+enum {
+	IB_USER_MAD_USER_RMPP = (1 << 0),
+};
+#define IB_USER_MAD_REG_FLAGS_CAP (IB_USER_MAD_USER_RMPP)
+struct ib_user_mad_reg_req2 {
+	__u32	id;
+	__u32	qpn;
+	__u8	mgmt_class;
+	__u8	mgmt_class_version;
+	__u16   res;
+	__u32   flags;
+	__u64   method_mask[2];
+	__u32   oui;
+	__u8	rmpp_version;
+	__u8	reserved[3];
+};
+
 #define IB_IOCTL_MAGIC		0x1b
 
 #define IB_USER_MAD_REGISTER_AGENT	_IOWR(IB_IOCTL_MAGIC, 1, \
@@ -200,4 +239,7 @@
 
 #define IB_USER_MAD_ENABLE_PKEY		_IO(IB_IOCTL_MAGIC, 3)
 
+#define IB_USER_MAD_REGISTER_AGENT2     _IOWR(IB_IOCTL_MAGIC, 4, \
+					      struct ib_user_mad_reg_req2)
+
 #endif /* IB_USER_MAD_H */
--- a/hfi1/mad.c
+++ b/hfi1/mad.c
@@ -121,7 +121,7 @@
 
 	send_buf = ib_create_send_mad(agent, qpn, pkey_idx, 0,
 				      IB_MGMT_MAD_HDR, IB_MGMT_MAD_DATA,
-				      GFP_ATOMIC, IB_MGMT_BASE_VERSION);
+					  GFP_ATOMIC);
 	if (IS_ERR(send_buf))
 		return;
 
@@ -4309,13 +4309,13 @@
 				u8 port, const struct ib_wc *in_wc,
 				const struct ib_grh *in_grh,
 				const struct opa_mad *in_mad,
-				struct opa_mad *out_mad, size_t *out_mad_size,
-				u16 *out_mad_pkey_index)
+				struct opa_mad *out_mad)
 {
 	int ret;
 	int pkey_idx;
 	u32 resp_len = 0;
 	struct hfi1_ibport *ibp = to_iport(ibdev, port);
+	struct ib_wc *in_out_wc = (struct ib_wc *)in_wc;
 
 	pkey_idx = hfi1_lookup_pkey_idx(ibp, LIM_MGMT_P_KEY);
 	if (pkey_idx < 0) {
@@ -4323,7 +4323,7 @@
 			hfi1_get_pkey(ibp, 1));
 		pkey_idx = 1;
 	}
-	*out_mad_pkey_index = (u16)pkey_idx;
+	in_out_wc->pkey_index = (u16)pkey_idx;
 
 	switch (in_mad->mad_hdr.mgmt_class) {
 	case IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE:
@@ -4347,9 +4347,9 @@
 
 bail:
 	if (ret & IB_MAD_RESULT_REPLY)
-		*out_mad_size = round_up(resp_len, 8);
+		in_out_wc->byte_len = round_up(resp_len, 8);
 	else if (ret & IB_MAD_RESULT_SUCCESS)
-		*out_mad_size = in_wc->byte_len - sizeof(struct ib_grh);
+		in_out_wc->byte_len -= sizeof(struct ib_grh);
 
 	return ret;
 }
@@ -4398,28 +4398,21 @@
  * This is called by the ib_mad module.
  */
 int hfi1_process_mad(struct ib_device *ibdev, int mad_flags, u8 port,
-		     const struct ib_wc *in_wc, const struct ib_grh *in_grh,
-		     const struct ib_mad_hdr *in_mad, size_t in_mad_size,
-		     struct ib_mad_hdr *out_mad, size_t *out_mad_size,
-		     u16 *out_mad_pkey_index)
+		     struct ib_wc *in_wc, struct ib_grh *in_grh,
+		     struct ib_mad *in_mad, struct ib_mad *out_mad)
 {
-	switch (in_mad->base_version) {
+	switch (in_mad->mad_hdr.base_version) {
 	case OPA_MGMT_BASE_VERSION:
-		if (unlikely(in_mad_size != sizeof(struct opa_mad))) {
-			dev_err(ibdev->dma_device, "invalid in_mad_size\n");
-			return IB_MAD_RESULT_FAILURE;
-		}
-		return hfi1_process_opa_mad(ibdev, mad_flags, port,
-					    in_wc, in_grh,
-					    (struct opa_mad *)in_mad,
-					    (struct opa_mad *)out_mad,
-					    out_mad_size,
-					    out_mad_pkey_index);
+		return hfi1_process_opa_mad(ibdev, mad_flags, port, in_wc,
+					    (const struct ib_grh *)in_grh,
+					    (const struct opa_mad *)in_mad,
+					    (struct opa_mad *)out_mad);
 	case IB_MGMT_BASE_VERSION:
 		return hfi1_process_ib_mad(ibdev, mad_flags, port,
-					  in_wc, in_grh,
-					  (const struct ib_mad *)in_mad,
-					  (struct ib_mad *)out_mad);
+					   (const struct ib_wc *)in_wc,
+					   (const struct ib_grh *)in_grh,
+					   (const struct ib_mad *)in_mad,
+					   out_mad);
 	default:
 		break;
 	}
--- a/include/rdma/opa_smi.h
+++ b/include/rdma/opa_smi.h
@@ -44,6 +44,8 @@
 #define OPA_MAX_SLS				32
 #define OPA_MAX_SCS				32
 
+#define OPA_SMI_CLASS_VERSION			0x80
+
 #define OPA_LID_PERMISSIVE			cpu_to_be32(0xFFFFFFFF)
 
 struct opa_smp {
--- a/hfi1/verbs.h
+++ b/hfi1/verbs.h
@@ -334,10 +334,8 @@
 void hfi1_sys_guid_chg(struct hfi1_ibport *ibp);
 void hfi1_node_desc_chg(struct hfi1_ibport *ibp);
 int hfi1_process_mad(struct ib_device *ibdev, int mad_flags, u8 port,
-		     const struct ib_wc *in_wc, const struct ib_grh *in_grh,
-		     const struct ib_mad_hdr *in_mad, size_t in_mad_size,
-		     struct ib_mad_hdr *out_mad, size_t *out_mad_size,
-		     u16 *out_mad_pkey_index);
+		     struct ib_wc *in_wc, struct ib_grh *in_grh,
+		  	 struct ib_mad *in_mad, struct ib_mad *out_mad);
 
 /*
  * The PSN_MASK and PSN_SHIFT allow for
--- a/hfi1/verbs.c
+++ b/hfi1/verbs.c
@@ -1501,6 +1501,7 @@
 			IB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |
 			IB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |
 			IB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE |
+		    IB_DEVICE_OPA_MAD_SUPPORT |
 			IB_DEVICE_MEM_MGT_EXTENSIONS;
 	rdi->dparms.props.page_size_cap = PAGE_SIZE;
 	rdi->dparms.props.vendor_id = dd->oui1 << 16 | dd->oui2 << 8 | dd->oui3;
--- a/ib_mad/Makefile
+++ b/ib_mad/Makefile
@@ -14,7 +14,7 @@
 
 obj-$(CONFIG_INFINIBAND) += ib_mad.o
 
-ib_mad-y := mad.o smi.o agent.o mad_rmpp.o
+ib_mad-y := mad.o smi.o agent.o mad_rmpp.o opa_mad.o opa_mad_rmpp.o opa_smi.o
 
 else
 #normal makefile
--- a/ib_mad/agent.c
+++ b/ib_mad/agent.c
@@ -54,7 +54,7 @@
 static LIST_HEAD(ib_agent_port_list);
 
 static struct ib_agent_port_private *
-__ib_get_agent_port(const struct ib_device *device, int port_num)
+__ib_get_agent_port(struct ib_device *device, int port_num)
 {
 	struct ib_agent_port_private *entry;
 
@@ -67,7 +67,7 @@
 }
 
 static struct ib_agent_port_private *
-ib_get_agent_port(const struct ib_device *device, int port_num)
+ib_get_agent_port(struct ib_device *device, int port_num)
 {
 	struct ib_agent_port_private *entry;
 	unsigned long flags;
@@ -78,59 +78,118 @@
 	return entry;
 }
 
-void agent_send_response(const struct ib_mad_hdr *mad_hdr, const struct ib_grh *grh,
-			 const struct ib_wc *wc, const struct ib_device *device,
-			 int port_num, int qpn, size_t resp_mad_len, bool opa)
+static int get_agent_ah(struct ib_device *device, int port_num,
+			struct ib_grh *grh, struct ib_wc *wc, int qpn,
+			struct ib_mad_agent **agent, struct ib_ah **ah)
 {
 	struct ib_agent_port_private *port_priv;
-	struct ib_mad_agent *agent;
-	struct ib_mad_send_buf *send_buf;
-	struct ib_ah *ah;
-	struct ib_mad_send_wr_private *mad_send_wr;
-
-	if (rdma_cap_ib_switch(device))
+	if (device->node_type == RDMA_NODE_IB_SWITCH)
 		port_priv = ib_get_agent_port(device, 0);
 	else
 		port_priv = ib_get_agent_port(device, port_num);
 
 	if (!port_priv) {
 		dev_err(&device->dev, "Unable to find port agent\n");
-		return;
+		return 1;
 	}
 
-	agent = port_priv->agent[qpn];
-	ah = ib_create_ah_from_wc(agent->qp->pd, wc, grh, port_num);
-	if (IS_ERR(ah)) {
+	*agent = port_priv->agent[qpn];
+	*ah = ib_create_ah_from_wc((*agent)->qp->pd, wc, grh, port_num);
+	if (IS_ERR(*ah)) {
 		dev_err(&device->dev, "ib_create_ah_from_wc error %ld\n",
 			PTR_ERR(ah));
-		return;
+		return 1;
 	}
+	return 0;
+}
+
+void agent_send_response(struct ib_mad *mad, struct ib_grh *grh,
+			 struct ib_wc *wc, struct ib_device *device,
+			 int port_num, int qpn)
+{
+	struct ib_mad_agent *agent;
+	struct ib_mad_send_buf *send_buf;
+	struct ib_ah *ah;
+	struct ib_mad_send_wr_private *mad_send_wr;
 
-	if (opa && mad_hdr->base_version != OPA_MGMT_BASE_VERSION)
-		resp_mad_len = IB_MGMT_MAD_SIZE;
+	if (get_agent_ah(device, port_num, grh, wc, qpn, &agent, &ah))
+		return;
 
 	send_buf = ib_create_send_mad(agent, wc->src_qp, wc->pkey_index, 0,
-				      IB_MGMT_MAD_HDR,
-				      resp_mad_len - IB_MGMT_MAD_HDR,
-				      GFP_KERNEL,
-				      mad_hdr->base_version);
+				      IB_MGMT_MAD_HDR, IB_MGMT_MAD_DATA,
+				      GFP_KERNEL);
+	if (IS_ERR(send_buf)) {
+		printk(KERN_ERR SPFX "ib_create_send_mad error\n");
+		goto err1;
+	}
+
+	memcpy(send_buf->mad, mad, sizeof *mad);
+	send_buf->ah = ah;
+
+	if (device->node_type == RDMA_NODE_IB_SWITCH) {
+		mad_send_wr = container_of(send_buf,
+					   struct ib_mad_send_wr_private,
+					   send_buf);
+		mad_send_wr->send_wr.wr.ud.port_num = port_num;
+	}
+
+	if (ib_post_send_mad(send_buf, NULL)) {
+		printk(KERN_ERR SPFX "ib_post_send_mad error\n");
+		goto err2;
+	}
+	return;
+err2:
+	ib_free_send_mad(send_buf);
+err1:
+	ib_destroy_ah(ah);
+}
+
+/* FIXME merge with agent_send_response */
+void agent_send_opa_response(struct opa_mad *mad, struct ib_grh *grh,
+			 struct ib_wc *wc, struct ib_device *device,
+			 int port_num, int qpn, u32 resp_len)
+{
+	struct ib_mad_agent *agent;
+	struct ib_mad_send_buf *send_buf;
+	struct ib_ah *ah;
+	//struct ib_mad_send_wr_private *mad_send_wr;
+	size_t data_len;
+	u8 base_version;
+
+	if (get_agent_ah(device, port_num, grh, wc, qpn, &agent, &ah))
+		return;
+
+	/* base version determines MAD size */
+	base_version = mad->mad_hdr.base_version;
+	if (base_version == OPA_MGMT_BASE_VERSION)
+		data_len = resp_len - OPA_MGMT_MAD_HDR;
+	else
+		data_len = IB_MGMT_MAD_DATA;
+
+	send_buf = ib_create_send_mad(agent, wc->src_qp | (base_version << 24),
+				      wc->pkey_index, 0,
+				      OPA_MGMT_MAD_HDR, data_len,
+				      GFP_KERNEL);
 	if (IS_ERR(send_buf)) {
-		dev_err(&device->dev, "ib_create_send_mad error\n");
+		pr_err("ib_create_send_mad error\n");
 		goto err1;
 	}
 
-	memcpy(send_buf->mad, mad_hdr, resp_mad_len);
+	memcpy(send_buf->mad, mad, OPA_MGMT_MAD_HDR + data_len);
 	send_buf->ah = ah;
 
-	if (rdma_cap_ib_switch(device)) {
+/* FIXME for upstream
+ * embedded linux on a switch?
+	if (device->node_type == RDMA_NODE_IB_SWITCH) {
 		mad_send_wr = container_of(send_buf,
 					   struct ib_mad_send_wr_private,
 					   send_buf);
 		mad_send_wr->send_wr.wr.ud.port_num = port_num;
 	}
+*/
 
 	if (ib_post_send_mad(send_buf, NULL)) {
-		dev_err(&device->dev, "ib_post_send_mad error\n");
+		pr_err("ib_post_send_mad error\n");
 		goto err2;
 	}
 	return;
@@ -140,6 +199,7 @@
 	ib_destroy_ah(ah);
 }
 
+
 static void agent_send_handler(struct ib_mad_agent *mad_agent,
 			       struct ib_mad_send_wc *mad_send_wc)
 {
@@ -156,17 +216,17 @@
 	/* Create new device info */
 	port_priv = kzalloc(sizeof *port_priv, GFP_KERNEL);
 	if (!port_priv) {
-		dev_err(&device->dev, "No memory for ib_agent_port_private\n");
+		printk(KERN_ERR SPFX "No memory for ib_agent_port_private\n");
 		ret = -ENOMEM;
 		goto error1;
 	}
 
-	if (rdma_cap_ib_smi(device, port_num)) {
+	if (rdma_port_get_link_layer(device, port_num) == IB_LINK_LAYER_INFINIBAND) {
 		/* Obtain send only MAD agent for SMI QP */
 		port_priv->agent[0] = ib_register_mad_agent(device, port_num,
 							    IB_QPT_SMI, NULL, 0,
 							    &agent_send_handler,
-							    NULL, NULL, 0);
+							    NULL, NULL);
 		if (IS_ERR(port_priv->agent[0])) {
 			ret = PTR_ERR(port_priv->agent[0]);
 			goto error2;
@@ -177,7 +237,7 @@
 	port_priv->agent[1] = ib_register_mad_agent(device, port_num,
 						    IB_QPT_GSI, NULL, 0,
 						    &agent_send_handler,
-						    NULL, NULL, 0);
+						    NULL, NULL);
 	if (IS_ERR(port_priv->agent[1])) {
 		ret = PTR_ERR(port_priv->agent[1]);
 		goto error3;
@@ -207,7 +267,7 @@
 	port_priv = __ib_get_agent_port(device, port_num);
 	if (port_priv == NULL) {
 		spin_unlock_irqrestore(&ib_agent_port_list_lock, flags);
-		dev_err(&device->dev, "Port %d not found\n", port_num);
+		printk(KERN_ERR SPFX "Port %d not found\n", port_num);
 		return -ENODEV;
 	}
 	list_del(&port_priv->port_list);
--- a/ib_mad/agent.h
+++ b/ib_mad/agent.h
@@ -44,8 +44,12 @@
 
 extern int ib_agent_port_close(struct ib_device *device, int port_num);
 
-extern void agent_send_response(const struct ib_mad_hdr *mad_hdr, const struct ib_grh *grh,
-				const struct ib_wc *wc, const struct ib_device *device,
-				int port_num, int qpn, size_t resp_mad_len, bool opa);
+extern void agent_send_response(struct ib_mad *mad, struct ib_grh *grh,
+				struct ib_wc *wc, struct ib_device *device,
+				int port_num, int qpn);
+
+extern void agent_send_opa_response(struct opa_mad *mad, struct ib_grh *grh,
+				struct ib_wc *wc, struct ib_device *device,
+				int port_num, int qpn, u32 resp_len);
 
 #endif	/* __AGENT_H_ */
--- a/ib_mad/mad.c
+++ b/ib_mad/mad.c
@@ -3,7 +3,6 @@
  * Copyright (c) 2005 Intel Corporation.  All rights reserved.
  * Copyright (c) 2005 Mellanox Technologies Ltd.  All rights reserved.
  * Copyright (c) 2009 HNR Consulting. All rights reserved.
- * Copyright (c) 2014 Intel Corporation.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -34,9 +33,6 @@
  * SOFTWARE.
  *
  */
-
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
-
 #include <linux/dma-mapping.h>
 #include <linux/slab.h>
 #include <linux/module.h>
@@ -47,20 +43,36 @@
 #include "smi.h"
 #include "opa_smi.h"
 #include "agent.h"
+#include "opa_mad.h"
+#include "opa_mad_rmpp.h"
 
 MODULE_LICENSE("Dual BSD/GPL");
 MODULE_DESCRIPTION("kernel IB MAD API");
 MODULE_AUTHOR("Hal Rosenstock");
 MODULE_AUTHOR("Sean Hefty");
+MODULE_AUTHOR("Ira Weiny");
 
 static int mad_sendq_size = IB_MAD_QP_SEND_SIZE;
 static int mad_recvq_size = IB_MAD_QP_RECV_SIZE;
+/* FIXME add opa_recvq_size mod param */
 
 module_param_named(send_queue_size, mad_sendq_size, int, 0444);
 MODULE_PARM_DESC(send_queue_size, "Size of send queue in number of work requests");
 module_param_named(recv_queue_size, mad_recvq_size, int, 0444);
 MODULE_PARM_DESC(recv_queue_size, "Size of receive queue in number of work requests");
 
+static int mad_support_opa = 1;
+module_param_named(support_opa, mad_support_opa, int, 0444);
+MODULE_PARM_DESC(support_opa,
+	"Enable Opa MAD support on devices which support them (default 1)");
+
+static int mad_fix_opa_size = 0;
+module_param_named(fix_opa_size, mad_fix_opa_size, int, S_IRUGO | S_IWUSR | S_IWGRP);
+MODULE_PARM_DESC(fix_opa_size, "Fix Opa MAD's to be 2K (default 0)");
+
+static struct kmem_cache *ib_mad_cache;
+struct kmem_cache *opa_mad_cache;
+
 /*
  * Define a limit on the number of completions which will be processed by the
  * worker thread in a single work item.  This ensures that other work items
@@ -83,9 +95,6 @@
 static int method_in_use(struct ib_mad_mgmt_method_table **method,
 			 struct ib_mad_reg_req *mad_reg_req);
 static void remove_mad_reg_req(struct ib_mad_agent_private *priv);
-static struct ib_mad_agent_private *find_mad_agent(
-					struct ib_mad_port_private *port_priv,
-					const struct ib_mad_hdr *mad);
 static int ib_mad_post_receive_mads(struct ib_mad_qp_info *qp_info,
 				    struct ib_mad_private *mad);
 static void cancel_mads(struct ib_mad_agent_private *mad_agent_priv);
@@ -191,12 +200,12 @@
 	return 0;
 }
 
-int ib_response_mad(const struct ib_mad_hdr *hdr)
+int ib_response_mad(struct ib_mad *mad)
 {
-	return ((hdr->method & IB_MGMT_METHOD_RESP) ||
-		(hdr->method == IB_MGMT_METHOD_TRAP_REPRESS) ||
-		((hdr->mgmt_class == IB_MGMT_CLASS_BM) &&
-		 (hdr->attr_mod & IB_BM_ATTR_MOD_RESP)));
+	return ((mad->mad_hdr.method & IB_MGMT_METHOD_RESP) ||
+		(mad->mad_hdr.method == IB_MGMT_METHOD_TRAP_REPRESS) ||
+		((mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_BM) &&
+		 (mad->mad_hdr.attr_mod & IB_BM_ATTR_MOD_RESP)));
 }
 EXPORT_SYMBOL(ib_response_mad);
 
@@ -207,11 +216,10 @@
 					   u8 port_num,
 					   enum ib_qp_type qp_type,
 					   struct ib_mad_reg_req *mad_reg_req,
-					   u8 rmpp_version,
+					   u8 rmpp_flags,
 					   ib_mad_send_handler send_handler,
 					   ib_mad_recv_handler recv_handler,
-					   void *context,
-					   u32 registration_flags)
+					   void *context)
 {
 	struct ib_mad_port_private *port_priv;
 	struct ib_mad_agent *ret = ERR_PTR(-EINVAL);
@@ -224,6 +232,8 @@
 	int ret2, qpn;
 	unsigned long flags;
 	u8 mgmt_class, vclass;
+	u8 rmpp_version = RMPP_VERSION(rmpp_flags);
+	u32 registration_flags = RMPP_FLAGS(rmpp_flags);
 
 	/* Validate parameters */
 	qpn = get_spl_qp_index(qp_type);
@@ -295,13 +305,12 @@
 				goto error1;
 			}
 		}
-
 		/* Make sure class supplied is consistent with QP type */
 		if (qp_type == IB_QPT_SMI) {
 			if ((mad_reg_req->mgmt_class !=
 					IB_MGMT_CLASS_SUBN_LID_ROUTED) &&
 			    (mad_reg_req->mgmt_class !=
-					IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
+				 IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
 				dev_notice(&device->dev,
 					   "ib_register_mad_agent: Invalid SM QP type: class 0x%x\n",
 					   mad_reg_req->mgmt_class);
@@ -311,7 +320,7 @@
 			if ((mad_reg_req->mgmt_class ==
 					IB_MGMT_CLASS_SUBN_LID_ROUTED) ||
 			    (mad_reg_req->mgmt_class ==
-					IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
+				 IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
 				dev_notice(&device->dev,
 					   "ib_register_mad_agent: Invalid GS QP type: class 0x%x\n",
 					   mad_reg_req->mgmt_class);
@@ -563,12 +572,6 @@
 }
 EXPORT_SYMBOL(ib_register_mad_snoop);
 
-static inline void deref_mad_agent(struct ib_mad_agent_private *mad_agent_priv)
-{
-	if (atomic_dec_and_test(&mad_agent_priv->refcount))
-		complete(&mad_agent_priv->comp);
-}
-
 static inline void deref_snoop_agent(struct ib_mad_snoop_private *mad_snoop_priv)
 {
 	if (atomic_dec_and_test(&mad_snoop_priv->refcount))
@@ -647,7 +650,7 @@
 }
 EXPORT_SYMBOL(ib_unregister_mad_agent);
 
-static void dequeue_mad(struct ib_mad_list_head *mad_list)
+void dequeue_mad(struct ib_mad_list_head *mad_list)
 {
 	struct ib_mad_queue *mad_queue;
 	unsigned long flags;
@@ -686,7 +689,7 @@
 	spin_unlock_irqrestore(&qp_info->snoop_lock, flags);
 }
 
-static void snoop_recv(struct ib_mad_qp_info *qp_info,
+void snoop_recv(struct ib_mad_qp_info *qp_info,
 		       struct ib_mad_recv_wc *mad_recv_wc,
 		       int mad_snoop_flags)
 {
@@ -729,32 +732,6 @@
 	wc->port_num = port_num;
 }
 
-static size_t mad_priv_size(const struct ib_mad_private *mp)
-{
-	return sizeof(struct ib_mad_private) + mp->mad_size;
-}
-
-static struct ib_mad_private *alloc_mad_private(size_t mad_size, gfp_t flags)
-{
-	size_t size = sizeof(struct ib_mad_private) + mad_size;
-	struct ib_mad_private *ret = kzalloc(size, flags);
-
-	if (ret)
-		ret->mad_size = mad_size;
-
-	return ret;
-}
-
-static size_t port_mad_size(const struct ib_mad_port_private *port_priv)
-{
-	return rdma_max_mad_size(port_priv->device, port_priv->port_num);
-}
-
-static size_t mad_priv_dma_size(const struct ib_mad_private *mp)
-{
-	return sizeof(struct ib_grh) + mp->mad_size;
-}
-
 /*
  * Return 0 if SMP is to be sent
  * Return 1 if SMP was consumed locally (whether or not solicited)
@@ -768,20 +745,16 @@
 	struct opa_smp *opa_smp = (struct opa_smp *)smp;
 	unsigned long flags;
 	struct ib_mad_local_private *local;
-	struct ib_mad_private *mad_priv;
+	struct ib_mad_private *mad_priv; /* or opa_mad_priv */
 	struct ib_mad_port_private *port_priv;
 	struct ib_mad_agent_private *recv_mad_agent = NULL;
 	struct ib_device *device = mad_agent_priv->agent.device;
 	u8 port_num;
 	struct ib_wc mad_wc;
 	struct ib_send_wr *send_wr = &mad_send_wr->send_wr;
-	size_t mad_size = port_mad_size(mad_agent_priv->qp_info->port_priv);
-	u16 out_mad_pkey_index = 0;
-	u16 drslid;
-	bool opa = rdma_cap_opa_mad(mad_agent_priv->qp_info->port_priv->device,
-				    mad_agent_priv->qp_info->port_priv->port_num);
+	u32 opa_drslid;
 
-	if (rdma_cap_ib_switch(device) &&
+	if (device->node_type == RDMA_NODE_IB_SWITCH &&
 	    smp->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)
 		port_num = send_wr->wr.ud.port_num;
 	else
@@ -793,86 +766,90 @@
 	 * If we are at the start of the LID routed part, don't update the
 	 * hop_ptr or hop_cnt.  See section 14.2.2, Vol 1 IB spec.
 	 */
-	if (opa && smp->class_version == OPA_SMP_CLASS_VERSION) {
-		u32 opa_drslid;
-
+	if (smp->base_version == OPA_MGMT_BASE_VERSION) {
 		if ((opa_get_smp_direction(opa_smp)
 		     ? opa_smp->route.dr.dr_dlid : opa_smp->route.dr.dr_slid) ==
 		     OPA_LID_PERMISSIVE &&
-		     opa_smi_handle_dr_smp_send(opa_smp,
-						rdma_cap_ib_switch(device),
+		     opa_smi_handle_dr_smp_send(opa_smp, device->node_type,
 						port_num) == IB_SMI_DISCARD) {
 			ret = -EINVAL;
-			dev_err(&device->dev, "OPA Invalid directed route\n");
+			dev_err(&device->dev, "Invalid directed route\n");
 			goto out;
 		}
 		opa_drslid = be32_to_cpu(opa_smp->route.dr.dr_slid);
-		if (opa_drslid != be32_to_cpu(OPA_LID_PERMISSIVE) &&
+		if (opa_drslid != OPA_LID_PERMISSIVE &&
 		    opa_drslid & 0xffff0000) {
 			ret = -EINVAL;
-			dev_err(&device->dev, "OPA Invalid dr_slid 0x%x\n",
+			dev_err(&device->dev, "STL Invalid dr_slid 0x%x\n",
 			       opa_drslid);
 			goto out;
 		}
-		drslid = (u16)(opa_drslid & 0x0000ffff);
-
-		/* Check to post send on QP or process locally */
-		if (opa_smi_check_local_smp(opa_smp, device) == IB_SMI_DISCARD &&
-		    opa_smi_check_local_returning_smp(opa_smp, device) == IB_SMI_DISCARD)
-			goto out;
 	} else {
 		if ((ib_get_smp_direction(smp) ? smp->dr_dlid : smp->dr_slid) ==
 		     IB_LID_PERMISSIVE &&
-		     smi_handle_dr_smp_send(smp, rdma_cap_ib_switch(device), port_num) ==
+		     smi_handle_dr_smp_send(smp, device->node_type, port_num) ==
 		     IB_SMI_DISCARD) {
 			ret = -EINVAL;
 			dev_err(&device->dev, "Invalid directed route\n");
 			goto out;
 		}
-		drslid = be16_to_cpu(smp->dr_slid);
-
-		/* Check to post send on QP or process locally */
-		if (smi_check_local_smp(smp, device) == IB_SMI_DISCARD &&
-		    smi_check_local_returning_smp(smp, device) == IB_SMI_DISCARD)
-			goto out;
+		opa_drslid = be16_to_cpu(smp->dr_slid);
 	}
 
+	/* Check to post send on QP or process locally */
+	if (smi_check_local_smp(smp, device) == IB_SMI_DISCARD &&
+	    smi_check_local_returning_smp(smp, device) == IB_SMI_DISCARD)
+		goto out;
+
 	local = kmalloc(sizeof *local, GFP_ATOMIC);
 	if (!local) {
 		ret = -ENOMEM;
-		dev_err(&device->dev, "No memory for ib_mad_local_private\n");
+		printk(KERN_ERR PFX "No memory for ib_mad_local_private\n");
 		goto out;
 	}
 	local->mad_priv = NULL;
 	local->recv_mad_agent = NULL;
-	mad_priv = alloc_mad_private(mad_size, GFP_ATOMIC);
+
+	if (mad_agent_priv->qp_info->supports_opa_mads)
+		mad_priv = kmem_cache_alloc(opa_mad_cache, GFP_ATOMIC);
+	else
+		mad_priv = kmem_cache_alloc(ib_mad_cache, GFP_ATOMIC);
+
 	if (!mad_priv) {
 		ret = -ENOMEM;
-		dev_err(&device->dev, "No memory for local response MAD\n");
+		printk(KERN_ERR PFX "No memory for local response MAD\n");
 		kfree(local);
 		goto out;
 	}
+	mad_priv->header.flags = 0;
+	if (mad_agent_priv->qp_info->supports_opa_mads)
+		mad_priv->header.flags = IB_MAD_PRIV_FLAG_OPA;
 
 	build_smp_wc(mad_agent_priv->agent.qp,
-		     send_wr->wr_id, drslid,
+		     send_wr->wr_id, (u16)(opa_drslid & 0x0000ffff),
 		     send_wr->wr.ud.pkey_index,
 		     send_wr->wr.ud.port_num, &mad_wc);
 
-	if (opa && smp->base_version == OPA_MGMT_BASE_VERSION) {
+	if (smp->base_version == OPA_MGMT_BASE_VERSION) {
 		mad_wc.byte_len = mad_send_wr->send_buf.hdr_len
 					+ mad_send_wr->send_buf.data_len
 					+ sizeof(struct ib_grh);
 	}
 
 	/* No GRH for DR SMP */
+	/* FIXME for upstream:
+	 * Once again drivers which support opa MADS know we will be passing them
+	 * a opa mad for the response.
+	 * Also we need to add a parameter here for the response length
+	 * For now we use the WC.byte_len
+	 */
 	ret = device->process_mad(device, 0, port_num, &mad_wc, NULL,
-				  (const struct ib_mad_hdr *)smp, mad_size,
-				  (struct ib_mad_hdr *)mad_priv->mad,
-				  &mad_size, &out_mad_pkey_index);
+				  (struct ib_mad *)smp,
+				  (struct ib_mad *)&mad_priv->mad);
 	switch (ret)
 	{
 	case IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_REPLY:
-		if (ib_response_mad((const struct ib_mad_hdr *)mad_priv->mad) &&
+		if (ib_response_mad(&mad_priv->mad.mad) &&
 		    mad_agent_priv->agent.recv_handler) {
 			local->mad_priv = mad_priv;
 			local->recv_mad_agent = mad_agent_priv;
@@ -881,44 +858,61 @@
 			 * side of local completion handled
 			 */
 			atomic_inc(&mad_agent_priv->refcount);
-		} else
-			kfree(mad_priv);
+		} else {
+			if (mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+				kmem_cache_free(opa_mad_cache, mad_priv);
+			else
+				kmem_cache_free(ib_mad_cache, mad_priv);
+		}
 		break;
 	case IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_CONSUMED:
-		kfree(mad_priv);
+		if (mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+			kmem_cache_free(opa_mad_cache, mad_priv);
+		else
+			kmem_cache_free(ib_mad_cache, mad_priv);
 		break;
 	case IB_MAD_RESULT_SUCCESS:
 		/* Treat like an incoming receive MAD */
 		port_priv = ib_get_mad_port(mad_agent_priv->agent.device,
 					    mad_agent_priv->agent.port_num);
 		if (port_priv) {
-			memcpy(mad_priv->mad, smp, mad_priv->mad_size);
+			memcpy(&mad_priv->mad.mad, smp, sizeof(struct ib_mad));
 			recv_mad_agent = find_mad_agent(port_priv,
-						        (const struct ib_mad_hdr *)mad_priv->mad);
+						        &mad_priv->mad.mad);
 		}
 		if (!port_priv || !recv_mad_agent) {
 			/*
 			 * No receiving agent so drop packet and
 			 * generate send completion.
 			 */
-			kfree(mad_priv);
+			if (mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+				kmem_cache_free(opa_mad_cache, mad_priv);
+			else
+				kmem_cache_free(ib_mad_cache, mad_priv);
 			break;
 		}
 		local->mad_priv = mad_priv;
 		local->recv_mad_agent = recv_mad_agent;
 		break;
 	default:
-		kfree(mad_priv);
+		if (mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+			kmem_cache_free(opa_mad_cache, mad_priv);
+		else
+			kmem_cache_free(ib_mad_cache, mad_priv);
 		kfree(local);
 		ret = -EINVAL;
 		goto out;
 	}
 
 	local->mad_send_wr = mad_send_wr;
-	if (opa) {
-		local->mad_send_wr->send_wr.wr.ud.pkey_index = out_mad_pkey_index;
-		local->return_wc_byte_len = mad_size;
-	}
+	/* FIXME upstream; pkey_index valid for IB ??? */
+	local->mad_send_wr->send_wr.wr.ud.pkey_index = mad_wc.pkey_index;
+	/* FIXME upstream; mad_wc.byte_len should be additional param in
+	 * process_mad */
+	if (mad_fix_opa_size)
+		local->return_wc_byte_len = sizeof(struct opa_mad);
+	else
+		local->return_wc_byte_len = mad_wc.byte_len;
 	/* Reference MAD agent until send side of local completion handled */
 	atomic_inc(&mad_agent_priv->refcount);
 	/* Queue local completion to local list */
@@ -955,15 +949,22 @@
 }
 
 static int alloc_send_rmpp_list(struct ib_mad_send_wr_private *send_wr,
-				size_t mad_size, gfp_t gfp_mask)
+				gfp_t gfp_mask, size_t mad_size)
 {
+/**
+ * FIXME this is wrong if we are a opa device sending to a non-opa device
+ * User will need to specify if they want the RMPP to be segmented on opa or
+ * IB MAD boundaries.  Specification mechanism is TBD...
+ *
+ * FIXED == mad_size will only be larger when base_version is
+ * OPA_MGMT_BASE_VERSION as specified by our caller
+ */
 	struct ib_mad_send_buf *send_buf = &send_wr->send_buf;
-	struct ib_rmpp_mad *rmpp_mad = send_buf->mad;
+	struct ib_rmpp_base *rmpp_base = send_buf->mad;
 	struct ib_rmpp_segment *seg = NULL;
 	int left, seg_size, pad;
 
 	send_buf->seg_size = mad_size - send_buf->hdr_len;
-	send_buf->seg_rmpp_size = mad_size - IB_MGMT_RMPP_HDR;
 	seg_size = send_buf->seg_size;
 	pad = send_wr->pad;
 
@@ -971,9 +972,9 @@
 	for (left = send_buf->data_len + pad; left > 0; left -= seg_size) {
 		seg = kmalloc(sizeof (*seg) + seg_size, gfp_mask);
 		if (!seg) {
-			dev_err(&send_buf->mad_agent->device->dev,
-				"alloc_send_rmpp_segs: RMPP mem alloc failed for len %zd, gfp %#x\n",
-				sizeof (*seg) + seg_size, gfp_mask);
+			printk(KERN_ERR "alloc_send_rmpp_segs: RMPP mem "
+			       "alloc failed for len %zd, gfp %#x\n",
+			       sizeof (*seg) + seg_size, gfp_mask);
 			free_send_rmpp_list(send_wr);
 			return -ENOMEM;
 		}
@@ -985,10 +986,10 @@
 	if (pad)
 		memset(seg->data + seg_size - pad, 0, pad);
 
-	rmpp_mad->rmpp_hdr.rmpp_version = send_wr->mad_agent_priv->
+	rmpp_base->rmpp_hdr.rmpp_version = send_wr->mad_agent_priv->
 					  agent.rmpp_version;
-	rmpp_mad->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_DATA;
-	ib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
+	rmpp_base->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_DATA;
+	ib_set_rmpp_flags(&rmpp_base->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
 
 	send_wr->cur_seg = container_of(send_wr->rmpp_list.next,
 					struct ib_rmpp_segment, list);
@@ -996,32 +997,43 @@
 	return 0;
 }
 
-int ib_mad_kernel_rmpp_agent(const struct ib_mad_agent *agent)
+int ib_mad_kernel_rmpp_agent(struct ib_mad_agent *agent)
 {
 	return agent->rmpp_version && !(agent->flags & IB_MAD_USER_RMPP);
 }
 EXPORT_SYMBOL(ib_mad_kernel_rmpp_agent);
 
+/**
+ * ib_create_send_mad needs some way to tell if the MAD/(RMPP seq) being created
+ * is to be opa or IB based.  The way to tell this is by the base_version
+ * specified.  Unfortunately, we don't have a parameter to specify the
+ * base_version and we don't want to break users outside of our modified
+ * modules.
+ *
+ * For now add the base_version as the upper byte of "remote_qpn".  This
+ * means that a value of 0x0 or 0x1 == IB MAD and 0x80 == Opa MAD.
+ *
+ * FIXME for upstream; add parameter for base_version
+ */
 struct ib_mad_send_buf * ib_create_send_mad(struct ib_mad_agent *mad_agent,
 					    u32 remote_qpn, u16 pkey_index,
 					    int rmpp_active,
 					    int hdr_len, int data_len,
-					    gfp_t gfp_mask,
-					    u8 base_version)
+					    gfp_t gfp_mask)
 {
 	struct ib_mad_agent_private *mad_agent_priv;
 	struct ib_mad_send_wr_private *mad_send_wr;
 	int pad, message_size, ret, size;
 	void *buf;
 	size_t mad_size;
-	bool opa;
+	u8 base_version = remote_qpn >> 24;
+	remote_qpn &= 0x00FFFFFF;
 
 	mad_agent_priv = container_of(mad_agent, struct ib_mad_agent_private,
 				      agent);
 
-	opa = rdma_cap_opa_mad(mad_agent->device, mad_agent->port_num);
-
-	if (opa && base_version == OPA_MGMT_BASE_VERSION)
+	if (mad_agent_priv->qp_info->supports_opa_mads
+	    && base_version == OPA_MGMT_BASE_VERSION)
 		mad_size = sizeof(struct opa_mad);
 	else
 		mad_size = sizeof(struct ib_mad);
@@ -1052,9 +1064,10 @@
 	mad_send_wr->sg_list[0].length = hdr_len;
 	mad_send_wr->sg_list[0].lkey = mad_agent->mr->lkey;
 
-	/* OPA MADs don't have to be the full 2048 bytes */
-	if (opa && base_version == OPA_MGMT_BASE_VERSION &&
-	    data_len < mad_size - hdr_len)
+	/* individual opa MADs don't have to be 2048 bytes */
+	if (mad_agent_priv->qp_info->supports_opa_mads
+	    && base_version == OPA_MGMT_BASE_VERSION
+	    && data_len < mad_size - hdr_len)
 		mad_send_wr->sg_list[1].length = data_len;
 	else
 		mad_send_wr->sg_list[1].length = mad_size - hdr_len;
@@ -1071,7 +1084,7 @@
 	mad_send_wr->send_wr.wr.ud.pkey_index = pkey_index;
 
 	if (rmpp_active) {
-		ret = alloc_send_rmpp_list(mad_send_wr, mad_size, gfp_mask);
+		ret = alloc_send_rmpp_list(mad_send_wr, gfp_mask, mad_size);
 		if (ret) {
 			kfree(buf);
 			return ERR_PTR(ret);
@@ -1281,6 +1294,15 @@
 		mad_send_wr->tid = ((struct ib_mad_hdr *) send_buf->mad)->tid;
 		/* Timeout will be updated after send completes */
 		mad_send_wr->timeout = msecs_to_jiffies(send_buf->timeout_ms);
+
+		/*
+		 * Add 1 sec to detect a send queue problem.
+		 * This will give send only MAD's 1 sec to get on the wire
+		 */
+		mad_send_wr->sq_timeout = msecs_to_jiffies(send_buf->timeout_ms);
+		mad_send_wr->sq_timeout += msecs_to_jiffies(1000);
+		mad_send_wr->sq_timeout += jiffies;
+
 		mad_send_wr->max_retries = send_buf->retries;
 		mad_send_wr->retries_left = send_buf->retries;
 		send_buf->retries = 0;
@@ -1341,7 +1363,10 @@
 					    recv_wc);
 		priv = container_of(mad_priv_hdr, struct ib_mad_private,
 				    header);
-		kfree(priv);
+		if (priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+			kmem_cache_free(opa_mad_cache, priv);
+		else
+			kmem_cache_free(ib_mad_cache, priv);
 	}
 }
 EXPORT_SYMBOL(ib_free_recv_mad);
@@ -1359,8 +1384,7 @@
 int ib_process_mad_wc(struct ib_mad_agent *mad_agent,
 		      struct ib_wc *wc)
 {
-	dev_err(&mad_agent->device->dev,
-		"ib_process_mad_wc() not implemented yet\n");
+	printk(KERN_ERR PFX "ib_process_mad_wc() not implemented yet\n");
 	return 0;
 }
 EXPORT_SYMBOL(ib_process_mad_wc);
@@ -1372,7 +1396,7 @@
 
 	for_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS) {
 		if ((*method)->agent[i]) {
-			pr_err("Method %d already in use\n", i);
+			printk(KERN_ERR PFX "Method %d already in use\n", i);
 			return -EINVAL;
 		}
 	}
@@ -1384,7 +1408,8 @@
 	/* Allocate management method table */
 	*method = kzalloc(sizeof **method, GFP_ATOMIC);
 	if (!*method) {
-		pr_err("No memory for ib_mad_mgmt_method_table\n");
+		printk(KERN_ERR PFX "No memory for "
+		       "ib_mad_mgmt_method_table\n");
 		return -ENOMEM;
 	}
 
@@ -1428,7 +1453,7 @@
 }
 
 static int find_vendor_oui(struct ib_mad_mgmt_vendor_class *vendor_class,
-			   const char *oui)
+			   char *oui)
 {
 	int i;
 
@@ -1479,8 +1504,8 @@
 		/* Allocate management class table for "new" class version */
 		*class = kzalloc(sizeof **class, GFP_ATOMIC);
 		if (!*class) {
-			dev_err(&agent_priv->agent.device->dev,
-				"No memory for ib_mad_mgmt_class_table\n");
+			printk(KERN_ERR PFX "No memory for "
+			       "ib_mad_mgmt_class_table\n");
 			ret = -ENOMEM;
 			goto error1;
 		}
@@ -1546,8 +1571,8 @@
 		/* Allocate mgmt vendor class table for "new" class version */
 		vendor = kzalloc(sizeof *vendor, GFP_ATOMIC);
 		if (!vendor) {
-			dev_err(&agent_priv->agent.device->dev,
-				"No memory for ib_mad_mgmt_vendor_class_table\n");
+			printk(KERN_ERR PFX "No memory for "
+			       "ib_mad_mgmt_vendor_class_table\n");
 			goto error1;
 		}
 
@@ -1557,8 +1582,8 @@
 		/* Allocate table for this management vendor class */
 		vendor_class = kzalloc(sizeof *vendor_class, GFP_ATOMIC);
 		if (!vendor_class) {
-			dev_err(&agent_priv->agent.device->dev,
-				"No memory for ib_mad_mgmt_vendor_class\n");
+			printk(KERN_ERR PFX "No memory for "
+			       "ib_mad_mgmt_vendor_class\n");
 			goto error2;
 		}
 
@@ -1589,7 +1614,7 @@
 			goto check_in_use;
 		}
 	}
-	dev_err(&agent_priv->agent.device->dev, "All OUI slots in use\n");
+	printk(KERN_ERR PFX "All OUI slots in use\n");
 	goto error3;
 
 check_in_use:
@@ -1724,15 +1749,15 @@
 	return;
 }
 
-static struct ib_mad_agent_private *
+struct ib_mad_agent_private *
 find_mad_agent(struct ib_mad_port_private *port_priv,
-	       const struct ib_mad_hdr *mad_hdr)
+	       struct ib_mad *mad)
 {
 	struct ib_mad_agent_private *mad_agent = NULL;
 	unsigned long flags;
 
 	spin_lock_irqsave(&port_priv->reg_lock, flags);
-	if (ib_response_mad(mad_hdr)) {
+	if (ib_response_mad(mad)) {
 		u32 hi_tid;
 		struct ib_mad_agent_private *entry;
 
@@ -1740,7 +1765,7 @@
 		 * Routing is based on high 32 bits of transaction ID
 		 * of MAD.
 		 */
-		hi_tid = be64_to_cpu(mad_hdr->tid) >> 32;
+		hi_tid = be64_to_cpu(mad->mad_hdr.tid) >> 32;
 		list_for_each_entry(entry, &port_priv->agent_list, agent_list) {
 			if (entry->agent.hi_tid == hi_tid) {
 				mad_agent = entry;
@@ -1752,45 +1777,45 @@
 		struct ib_mad_mgmt_method_table *method;
 		struct ib_mad_mgmt_vendor_class_table *vendor;
 		struct ib_mad_mgmt_vendor_class *vendor_class;
-		const struct ib_vendor_mad *vendor_mad;
+		struct ib_vendor_mad *vendor_mad;
 		int index;
 
 		/*
 		 * Routing is based on version, class, and method
 		 * For "newer" vendor MADs, also based on OUI
 		 */
-		if (mad_hdr->class_version >= MAX_MGMT_VERSION)
+		if (mad->mad_hdr.class_version >= MAX_MGMT_VERSION)
 			goto out;
-		if (!is_vendor_class(mad_hdr->mgmt_class)) {
+		if (!is_vendor_class(mad->mad_hdr.mgmt_class)) {
 			class = port_priv->version[
-					mad_hdr->class_version].class;
+					mad->mad_hdr.class_version].class;
 			if (!class)
 				goto out;
-			if (convert_mgmt_class(mad_hdr->mgmt_class) >=
+			if (convert_mgmt_class(mad->mad_hdr.mgmt_class) >=
 			    IB_MGMT_MAX_METHODS)
 				goto out;
 			method = class->method_table[convert_mgmt_class(
-							mad_hdr->mgmt_class)];
+							mad->mad_hdr.mgmt_class)];
 			if (method)
-				mad_agent = method->agent[mad_hdr->method &
+				mad_agent = method->agent[mad->mad_hdr.method &
 							  ~IB_MGMT_METHOD_RESP];
 		} else {
 			vendor = port_priv->version[
-					mad_hdr->class_version].vendor;
+					mad->mad_hdr.class_version].vendor;
 			if (!vendor)
 				goto out;
 			vendor_class = vendor->vendor_class[vendor_class_index(
-						mad_hdr->mgmt_class)];
+						mad->mad_hdr.mgmt_class)];
 			if (!vendor_class)
 				goto out;
 			/* Find matching OUI */
-			vendor_mad = (const struct ib_vendor_mad *)mad_hdr;
+			vendor_mad = (struct ib_vendor_mad *)mad;
 			index = find_vendor_oui(vendor_class, vendor_mad->oui);
 			if (index == -1)
 				goto out;
 			method = vendor_class->method_table[index];
 			if (method) {
-				mad_agent = method->agent[mad_hdr->method &
+				mad_agent = method->agent[mad->mad_hdr.method &
 							  ~IB_MGMT_METHOD_RESP];
 			}
 		}
@@ -1800,9 +1825,9 @@
 		if (mad_agent->agent.recv_handler)
 			atomic_inc(&mad_agent->refcount);
 		else {
-			dev_notice(&port_priv->device->dev,
-				   "No receive handler for client %p on port %d\n",
-				   &mad_agent->agent, port_priv->port_num);
+			printk(KERN_NOTICE PFX "No receive handler for client "
+			       "%p on port %d\n",
+			       &mad_agent->agent, port_priv->port_num);
 			mad_agent = NULL;
 		}
 	}
@@ -1812,24 +1837,20 @@
 	return mad_agent;
 }
 
-static int validate_mad(const struct ib_mad_hdr *mad_hdr,
-			const struct ib_mad_qp_info *qp_info,
-			bool opa)
+static int validate_mad(struct ib_mad *mad, u32 qp_num)
 {
 	int valid = 0;
-	u32 qp_num = qp_info->qp->qp_num;
 
 	/* Make sure MAD base version is understood */
-	if (mad_hdr->base_version != IB_MGMT_BASE_VERSION &&
-	    (!opa || mad_hdr->base_version != OPA_MGMT_BASE_VERSION)) {
-		pr_err("MAD received with unsupported base version %d %s\n",
-		       mad_hdr->base_version, opa ? "(opa)" : "");
+	if (mad->mad_hdr.base_version != IB_MGMT_BASE_VERSION) {
+		printk(KERN_ERR PFX "MAD received with unsupported base "
+		       "version %d\n", mad->mad_hdr.base_version);
 		goto out;
 	}
 
 	/* Filter SMI packets sent to other than QP0 */
-	if ((mad_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_LID_ROUTED) ||
-	    (mad_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
+	if ((mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_LID_ROUTED) ||
+	    (mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {
 		if (qp_num == 0)
 			valid = 1;
 	} else {
@@ -1842,29 +1863,29 @@
 	return valid;
 }
 
-static int is_rmpp_data_mad(const struct ib_mad_agent_private *mad_agent_priv,
-			    const struct ib_mad_hdr *mad_hdr)
+static int is_rmpp_data_mad(struct ib_mad_agent_private *mad_agent_priv,
+		       struct ib_mad_hdr *mad_hdr)
 {
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 
-	rmpp_mad = (struct ib_rmpp_mad *)mad_hdr;
+	rmpp_base = (struct ib_rmpp_base *)mad_hdr;
 	return !mad_agent_priv->agent.rmpp_version ||
 		!ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent) ||
-		!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &
+		!(ib_get_rmpp_flags(&rmpp_base->rmpp_hdr) &
 				    IB_MGMT_RMPP_FLAG_ACTIVE) ||
-		(rmpp_mad->rmpp_hdr.rmpp_type == IB_MGMT_RMPP_TYPE_DATA);
+		(rmpp_base->rmpp_hdr.rmpp_type == IB_MGMT_RMPP_TYPE_DATA);
 }
 
-static inline int rcv_has_same_class(const struct ib_mad_send_wr_private *wr,
-				     const struct ib_mad_recv_wc *rwc)
+static inline int rcv_has_same_class(struct ib_mad_send_wr_private *wr,
+				     struct ib_mad_recv_wc *rwc)
 {
-	return ((struct ib_mad_hdr *)(wr->send_buf.mad))->mgmt_class ==
+	return ((struct ib_mad *)(wr->send_buf.mad))->mad_hdr.mgmt_class ==
 		rwc->recv_buf.mad->mad_hdr.mgmt_class;
 }
 
-static inline int rcv_has_same_gid(const struct ib_mad_agent_private *mad_agent_priv,
-				   const struct ib_mad_send_wr_private *wr,
-				   const struct ib_mad_recv_wc *rwc )
+static inline int rcv_has_same_gid(struct ib_mad_agent_private *mad_agent_priv,
+				   struct ib_mad_send_wr_private *wr,
+				   struct ib_mad_recv_wc *rwc )
 {
 	struct ib_ah_attr attr;
 	u8 send_resp, rcv_resp;
@@ -1873,8 +1894,8 @@
 	u8 port_num = mad_agent_priv->agent.port_num;
 	u8 lmc;
 
-	send_resp = ib_response_mad((struct ib_mad_hdr *)wr->send_buf.mad);
-	rcv_resp = ib_response_mad(&rwc->recv_buf.mad->mad_hdr);
+	send_resp = ib_response_mad((struct ib_mad *)wr->send_buf.mad);
+	rcv_resp = ib_response_mad(rwc->recv_buf.mad);
 
 	if (send_resp == rcv_resp)
 		/* both requests, or both responses. GIDs different */
@@ -1919,13 +1940,13 @@
 }
 
 struct ib_mad_send_wr_private*
-ib_find_send_mad(const struct ib_mad_agent_private *mad_agent_priv,
-		 const struct ib_mad_recv_wc *wc)
+ib_find_send_mad(struct ib_mad_agent_private *mad_agent_priv,
+		 struct ib_mad_recv_wc *wc)
 {
 	struct ib_mad_send_wr_private *wr;
-	const struct ib_mad_hdr *mad_hdr;
+	struct ib_mad_hdr *mad_hdr;
 
-	mad_hdr = &wc->recv_buf.mad->mad_hdr;
+	mad_hdr = (struct ib_mad_hdr *)wc->recv_buf.mad;
 
 	list_for_each_entry(wr, &mad_agent_priv->wait_list, agent_list) {
 		if ((wr->tid == mad_hdr->tid) &&
@@ -1987,14 +2008,14 @@
 	}
 
 	/* Complete corresponding request */
-	if (ib_response_mad(&mad_recv_wc->recv_buf.mad->mad_hdr)) {
+	if (ib_response_mad(mad_recv_wc->recv_buf.mad)) {
 		spin_lock_irqsave(&mad_agent_priv->lock, flags);
 		mad_send_wr = ib_find_send_mad(mad_agent_priv, mad_recv_wc);
 		if (!mad_send_wr) {
 			spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
 			if (!ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)
 			   && ib_is_mad_class_rmpp(mad_recv_wc->recv_buf.mad->mad_hdr.mgmt_class)
-			   && (ib_get_rmpp_flags(&((struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad)->rmpp_hdr)
+			   && (ib_get_rmpp_flags(&((struct ib_rmpp_base *)mad_recv_wc->recv_buf.mad)->rmpp_hdr)
 					& IB_MGMT_RMPP_FLAG_ACTIVE)) {
 				/* user rmpp is in effect
 				 * and this is an active RMPP MAD
@@ -2032,231 +2053,124 @@
 	}
 }
 
-static enum smi_action handle_ib_smi(const struct ib_mad_port_private *port_priv,
-				     const struct ib_mad_qp_info *qp_info,
-				     const struct ib_wc *wc,
-				     int port_num,
-				     struct ib_mad_private *recv,
-				     struct ib_mad_private *response)
+enum smi_action handle_ib_smi(struct ib_mad_port_private *port_priv,
+			      struct ib_mad_qp_info *qp_info,
+			      struct ib_wc *wc,
+			      int port_num,
+			      struct ib_mad_private *recv,
+			      struct ib_mad_private *response)
 {
 	enum smi_forward_action retsmi;
-	struct ib_smp *smp = (struct ib_smp *)recv->mad;
 
-	if (smi_handle_dr_smp_recv(smp,
-				   rdma_cap_ib_switch(port_priv->device),
+	if (smi_handle_dr_smp_recv(&recv->mad.smp,
+				   port_priv->device->node_type,
 				   port_num,
 				   port_priv->device->phys_port_cnt) ==
 				   IB_SMI_DISCARD)
-		return IB_SMI_DISCARD;
+		return (IB_SMI_DISCARD);
 
-	retsmi = smi_check_forward_dr_smp(smp);
+	retsmi = smi_check_forward_dr_smp(&recv->mad.smp);
 	if (retsmi == IB_SMI_LOCAL)
-		return IB_SMI_HANDLE;
+		return (IB_SMI_HANDLE);
 
 	if (retsmi == IB_SMI_SEND) { /* don't forward */
-		if (smi_handle_dr_smp_send(smp,
-					   rdma_cap_ib_switch(port_priv->device),
+		if (smi_handle_dr_smp_send(&recv->mad.smp,
+					   port_priv->device->node_type,
 					   port_num) == IB_SMI_DISCARD)
-			return IB_SMI_DISCARD;
+			return (IB_SMI_DISCARD);
+
+		if (smi_check_local_smp(&recv->mad.smp, port_priv->device) == IB_SMI_DISCARD)
+			return (IB_SMI_DISCARD);
+	} else if (port_priv->device->node_type == RDMA_NODE_IB_SWITCH) {
+
+		/* FIXME for upstream */
+		BUG_ON(response->header.flags & IB_MAD_PRIV_FLAG_OPA);
 
-		if (smi_check_local_smp(smp, port_priv->device) == IB_SMI_DISCARD)
-			return IB_SMI_DISCARD;
-	} else if (rdma_cap_ib_switch(port_priv->device)) {
 		/* forward case for switches */
-		memcpy(response, recv, mad_priv_size(response));
+		memcpy(response, recv, sizeof(*response));
 		response->header.recv_wc.wc = &response->header.wc;
-		response->header.recv_wc.recv_buf.mad = (struct ib_mad *)response->mad;
+		response->header.recv_wc.recv_buf.mad = &response->mad.mad;
 		response->header.recv_wc.recv_buf.grh = &response->grh;
 
-		agent_send_response((const struct ib_mad_hdr *)response->mad,
+		agent_send_response(&response->mad.mad,
 				    &response->grh, wc,
 				    port_priv->device,
-				    smi_get_fwd_port(smp),
-				    qp_info->qp->qp_num,
-				    response->mad_size,
-				    false);
+				    smi_get_fwd_port(&recv->mad.smp),
+				    qp_info->qp->qp_num);
 
-		return IB_SMI_DISCARD;
+		return (IB_SMI_DISCARD);
 	}
-	return IB_SMI_HANDLE;
+	return (IB_SMI_HANDLE);
 }
 
-static bool generate_unmatched_resp(const struct ib_mad_private *recv,
-				    struct ib_mad_private *response,
-				    size_t *resp_len, bool opa)
+static bool generate_unmatched_resp(struct ib_mad_private *recv,
+				    struct ib_mad_private *response)
 {
-	const struct ib_mad_hdr *recv_hdr = (const struct ib_mad_hdr *)recv->mad;
-	struct ib_mad_hdr *resp_hdr = (struct ib_mad_hdr *)response->mad;
-
-	if (recv_hdr->method == IB_MGMT_METHOD_GET ||
-	    recv_hdr->method == IB_MGMT_METHOD_SET) {
-		memcpy(response, recv, mad_priv_size(response));
+	if (recv->mad.mad.mad_hdr.method == IB_MGMT_METHOD_GET ||
+	    recv->mad.mad.mad_hdr.method == IB_MGMT_METHOD_SET) {
+		memcpy(response, recv, sizeof *response);
 		response->header.recv_wc.wc = &response->header.wc;
-		response->header.recv_wc.recv_buf.mad = (struct ib_mad *)response->mad;
+		response->header.recv_wc.recv_buf.mad = &response->mad.mad;
 		response->header.recv_wc.recv_buf.grh = &response->grh;
-		resp_hdr->method = IB_MGMT_METHOD_GET_RESP;
-		resp_hdr->status = cpu_to_be16(IB_MGMT_MAD_STATUS_UNSUPPORTED_METHOD_ATTRIB);
-		if (recv_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)
-			resp_hdr->status |= IB_SMP_DIRECTION;
-
-		if (opa && recv_hdr->base_version == OPA_MGMT_BASE_VERSION) {
-			if (recv_hdr->mgmt_class ==
-			    IB_MGMT_CLASS_SUBN_LID_ROUTED ||
-			    recv_hdr->mgmt_class ==
-			    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)
-				*resp_len = opa_get_smp_header_size(
-							(struct opa_smp *)recv->mad);
-			else
-				*resp_len = sizeof(struct ib_mad_hdr);
-		}
+		response->mad.mad.mad_hdr.method = IB_MGMT_METHOD_GET_RESP;
+		response->mad.mad.mad_hdr.status =
+			cpu_to_be16(IB_MGMT_MAD_STATUS_UNSUPPORTED_METHOD_ATTRIB);
+		if (recv->mad.mad.mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)
+			response->mad.mad.mad_hdr.status |= IB_SMP_DIRECTION;
 
 		return true;
 	} else {
 		return false;
 	}
 }
-
-static enum smi_action
-handle_opa_smi(struct ib_mad_port_private *port_priv,
-	       struct ib_mad_qp_info *qp_info,
-	       struct ib_wc *wc,
-	       int port_num,
-	       struct ib_mad_private *recv,
-	       struct ib_mad_private *response)
-{
-	enum smi_forward_action retsmi;
-	struct opa_smp *smp = (struct opa_smp *)recv->mad;
-
-	if (opa_smi_handle_dr_smp_recv(smp,
-				   rdma_cap_ib_switch(port_priv->device),
-				   port_num,
-				   port_priv->device->phys_port_cnt) ==
-				   IB_SMI_DISCARD)
-		return IB_SMI_DISCARD;
-
-	retsmi = opa_smi_check_forward_dr_smp(smp);
-	if (retsmi == IB_SMI_LOCAL)
-		return IB_SMI_HANDLE;
-
-	if (retsmi == IB_SMI_SEND) { /* don't forward */
-		if (opa_smi_handle_dr_smp_send(smp,
-					   rdma_cap_ib_switch(port_priv->device),
-					   port_num) == IB_SMI_DISCARD)
-			return IB_SMI_DISCARD;
-
-		if (opa_smi_check_local_smp(smp, port_priv->device) ==
-		    IB_SMI_DISCARD)
-			return IB_SMI_DISCARD;
-
-	} else if (rdma_cap_ib_switch(port_priv->device)) {
-		/* forward case for switches */
-		memcpy(response, recv, mad_priv_size(response));
-		response->header.recv_wc.wc = &response->header.wc;
-		response->header.recv_wc.recv_buf.opa_mad =
-				(struct opa_mad *)response->mad;
-		response->header.recv_wc.recv_buf.grh = &response->grh;
-
-		agent_send_response((const struct ib_mad_hdr *)response->mad,
-				    &response->grh, wc,
-				    port_priv->device,
-				    opa_smi_get_fwd_port(smp),
-				    qp_info->qp->qp_num,
-				    recv->header.wc.byte_len,
-				    true);
-
-		return IB_SMI_DISCARD;
-	}
-
-	return IB_SMI_HANDLE;
-}
-
-static enum smi_action
-handle_smi(struct ib_mad_port_private *port_priv,
-	   struct ib_mad_qp_info *qp_info,
-	   struct ib_wc *wc,
-	   int port_num,
-	   struct ib_mad_private *recv,
-	   struct ib_mad_private *response,
-	   bool opa)
-{
-	struct ib_mad_hdr *mad_hdr = (struct ib_mad_hdr *)recv->mad;
-
-	if (opa && mad_hdr->base_version == OPA_MGMT_BASE_VERSION &&
-	    mad_hdr->class_version == OPA_SMI_CLASS_VERSION)
-		return handle_opa_smi(port_priv, qp_info, wc, port_num, recv,
-				      response);
-
-	return handle_ib_smi(port_priv, qp_info, wc, port_num, recv, response);
-}
-
 static void ib_mad_recv_done_handler(struct ib_mad_port_private *port_priv,
-				     struct ib_wc *wc)
+				     struct ib_wc *wc,
+				     struct ib_mad_private_header *mad_priv_hdr,
+				     struct ib_mad_qp_info *qp_info)
 {
-	struct ib_mad_qp_info *qp_info;
-	struct ib_mad_private_header *mad_priv_hdr;
 	struct ib_mad_private *recv, *response = NULL;
-	struct ib_mad_list_head *mad_list;
 	struct ib_mad_agent_private *mad_agent;
 	int port_num;
 	int ret = IB_MAD_RESULT_SUCCESS;
-	size_t mad_size;
-	u16 resp_mad_pkey_index = 0;
-	bool opa;
-
-	mad_list = (struct ib_mad_list_head *)(unsigned long)wc->wr_id;
-	qp_info = mad_list->mad_queue->qp_info;
-	dequeue_mad(mad_list);
 
-	opa = rdma_cap_opa_mad(qp_info->port_priv->device,
-			       qp_info->port_priv->port_num);
-
-	mad_priv_hdr = container_of(mad_list, struct ib_mad_private_header,
-				    mad_list);
 	recv = container_of(mad_priv_hdr, struct ib_mad_private, header);
 	ib_dma_unmap_single(port_priv->device,
 			    recv->header.mapping,
-			    mad_priv_dma_size(recv),
+			    sizeof(struct ib_mad_private) -
+			      sizeof(struct ib_mad_private_header),
 			    DMA_FROM_DEVICE);
 
 	/* Setup MAD receive work completion from "normal" work completion */
 	recv->header.wc = *wc;
 	recv->header.recv_wc.wc = &recv->header.wc;
-
-	if (opa && ((struct ib_mad_hdr *)(recv->mad))->base_version == OPA_MGMT_BASE_VERSION) {
-		recv->header.recv_wc.mad_len = wc->byte_len - sizeof(struct ib_grh);
-		recv->header.recv_wc.mad_seg_size = sizeof(struct opa_mad);
-	} else {
-		recv->header.recv_wc.mad_len = sizeof(struct ib_mad);
-		recv->header.recv_wc.mad_seg_size = sizeof(struct ib_mad);
-	}
-
-	recv->header.recv_wc.recv_buf.mad = (struct ib_mad *)recv->mad;
+	recv->header.recv_wc.mad_len = sizeof(struct ib_mad);
+	recv->header.recv_wc.recv_buf.mad = &recv->mad.mad;
 	recv->header.recv_wc.recv_buf.grh = &recv->grh;
 
 	if (atomic_read(&qp_info->snoop_count))
 		snoop_recv(qp_info, &recv->header.recv_wc, IB_MAD_SNOOP_RECVS);
 
 	/* Validate MAD */
-	if (!validate_mad((const struct ib_mad_hdr *)recv->mad, qp_info, opa))
+	if (!validate_mad(&recv->mad.mad, qp_info->qp->qp_num))
 		goto out;
 
-	mad_size = recv->mad_size;
-	response = alloc_mad_private(mad_size, GFP_KERNEL);
+	response = kmem_cache_alloc(ib_mad_cache, GFP_KERNEL);
 	if (!response) {
-		dev_err(&port_priv->device->dev,
-			"ib_mad_recv_done_handler no memory for response buffer\n");
+		printk(KERN_ERR PFX "ib_mad_recv_done_handler no memory "
+		       "for response buffer\n");
 		goto out;
 	}
+	response->header.flags = 0;
 
-	if (rdma_cap_ib_switch(port_priv->device))
+	if (port_priv->device->node_type == RDMA_NODE_IB_SWITCH)
 		port_num = wc->port_num;
 	else
 		port_num = port_priv->port_num;
 
-	if (((struct ib_mad_hdr *)recv->mad)->mgmt_class ==
+	if (recv->mad.mad.mad_hdr.mgmt_class ==
 	    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {
-		if (handle_smi(port_priv, qp_info, wc, port_num, recv,
-			       response, opa)
+		if (handle_ib_smi(port_priv, qp_info, wc, port_num, recv,
+				  response)
 		    == IB_SMI_DISCARD)
 			goto out;
 	}
@@ -2266,30 +2180,25 @@
 		ret = port_priv->device->process_mad(port_priv->device, 0,
 						     port_priv->port_num,
 						     wc, &recv->grh,
-						     (const struct ib_mad_hdr *)recv->mad,
-						     recv->mad_size,
-						     (struct ib_mad_hdr *)response->mad,
-						     &mad_size, &resp_mad_pkey_index);
-
-		if (opa)
-			wc->pkey_index = resp_mad_pkey_index;
-
+						     &recv->mad.mad,
+						     &response->mad.mad);
 		if (ret & IB_MAD_RESULT_SUCCESS) {
 			if (ret & IB_MAD_RESULT_CONSUMED)
 				goto out;
 			if (ret & IB_MAD_RESULT_REPLY) {
-				agent_send_response((const struct ib_mad_hdr *)response->mad,
+				if (mad_fix_opa_size)
+					wc->byte_len = sizeof(struct opa_mad);
+				agent_send_response(&response->mad.mad,
 						    &recv->grh, wc,
 						    port_priv->device,
 						    port_num,
-						    qp_info->qp->qp_num,
-						    mad_size, opa);
+						    qp_info->qp->qp_num);
 				goto out;
 			}
 		}
 	}
 
-	mad_agent = find_mad_agent(port_priv, (const struct ib_mad_hdr *)recv->mad);
+	mad_agent = find_mad_agent(port_priv, &recv->mad.mad);
 	if (mad_agent) {
 		ib_mad_complete_recv(mad_agent, &recv->header.recv_wc);
 		/*
@@ -2298,22 +2207,24 @@
 		 */
 		recv = NULL;
 	} else if ((ret & IB_MAD_RESULT_SUCCESS) &&
-		   generate_unmatched_resp(recv, response, &mad_size, opa)) {
-		agent_send_response((const struct ib_mad_hdr *)response->mad, &recv->grh, wc,
-				    port_priv->device, port_num,
-				    qp_info->qp->qp_num, mad_size, opa);
+		   generate_unmatched_resp(recv, response)) {
+		agent_send_response(&response->mad.mad, &recv->grh, wc,
+				    port_priv->device, port_num, qp_info->qp->qp_num);
 	}
 
 out:
 	/* Post another receive request for this QP */
 	if (response) {
 		ib_mad_post_receive_mads(qp_info, response);
-		kfree(recv);
+		if (recv) {
+			BUG_ON(recv->header.flags & IB_MAD_PRIV_FLAG_OPA);
+			kmem_cache_free(ib_mad_cache, recv);
+		}
 	} else
 		ib_mad_post_receive_mads(qp_info, recv);
 }
 
-static void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv)
+void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv)
 {
 	struct ib_mad_send_wr_private *mad_send_wr;
 	unsigned long delay;
@@ -2337,7 +2248,7 @@
 	}
 }
 
-static void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr)
+void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr)
 {
 	struct ib_mad_agent_private *mad_agent_priv;
 	struct ib_mad_send_wr_private *temp_mad_send_wr;
@@ -2482,8 +2393,7 @@
 		ret = ib_post_send(qp_info->qp, &queued_send_wr->send_wr,
 				   &bad_send_wr);
 		if (ret) {
-			dev_err(&port_priv->device->dev,
-				"ib_post_send failed: %d\n", ret);
+			printk(KERN_ERR PFX "ib_post_send failed: %d\n", ret);
 			mad_send_wr = queued_send_wr;
 			wc->status = IB_WC_LOC_QP_OP_ERR;
 			goto retry;
@@ -2555,9 +2465,8 @@
 					   IB_QP_STATE | IB_QP_CUR_STATE);
 			kfree(attr);
 			if (ret)
-				dev_err(&port_priv->device->dev,
-					"mad_error_handler - ib_modify_qp to RTS : %d\n",
-					ret);
+				printk(KERN_ERR PFX "mad_error_handler - "
+				       "ib_modify_qp to RTS : %d\n", ret);
 			else
 				mark_sends_for_retry(qp_info);
 		}
@@ -2565,6 +2474,26 @@
 	}
 }
 
+static void ib_mad_recv_mad(struct ib_mad_port_private *port_priv,
+			    struct ib_wc *wc)
+{
+	struct ib_mad_qp_info *qp_info;
+	struct ib_mad_list_head *mad_list;
+	struct ib_mad_private_header *mad_priv_hdr;
+
+	mad_list = (struct ib_mad_list_head *)(unsigned long)wc->wr_id;
+	qp_info = mad_list->mad_queue->qp_info;
+	dequeue_mad(mad_list);
+
+	mad_priv_hdr = container_of(mad_list, struct ib_mad_private_header,
+				    mad_list);
+
+	if (port_priv->supports_opa_mads)
+		ib_mad_recv_done_opa_handler(port_priv, wc, mad_priv_hdr, qp_info);
+	else
+		ib_mad_recv_done_handler(port_priv, wc, mad_priv_hdr, qp_info);
+}
+
 /*
  * IB MAD completion callback
  */
@@ -2584,7 +2513,7 @@
 				ib_mad_send_done_handler(port_priv, &wc);
 				break;
 			case IB_WC_RECV:
-				ib_mad_recv_done_handler(port_priv, &wc);
+				ib_mad_recv_mad(port_priv, &wc);
 				break;
 			default:
 				BUG_ON(1);
@@ -2600,6 +2529,48 @@
 	}
 }
 
+/**
+ * mad_agent_priv->lock should be held
+ */
+static int check_send_queue_stall(struct ib_mad_agent_private *mad_agent_priv)
+{
+	struct ib_mad_send_wr_private *mad_send_wr;
+	int stalled = 0;
+
+	if (list_empty(&mad_agent_priv->send_list)) {
+		if (unlikely(mad_agent_priv->qp_info->send_queue_stalled)) {
+			mad_agent_priv->qp_info->send_queue_stalled = 0;
+			pr_err("QP %d of %s no longer stalled\n",
+				mad_agent_priv->qp_info->qp->qp_num,
+				mad_agent_priv->qp_info->qp->device->name);
+		}
+		return 0;
+	}
+
+	mad_send_wr = list_entry(mad_agent_priv->send_list.next,
+				 struct ib_mad_send_wr_private,
+				 agent_list);
+
+	if (time_after(jiffies, mad_send_wr->sq_timeout)) {
+		struct ib_qp *stuck_qp;
+		struct ib_qp_attr qp_attr;
+		struct ib_qp_init_attr iqp_attr;
+
+		stuck_qp = mad_agent_priv->qp_info->qp;
+
+		ib_query_qp(stuck_qp, &qp_attr, IB_QP_STATE, &iqp_attr);
+
+		if (!mad_agent_priv->qp_info->send_queue_stalled)
+			pr_err("Detected stalled send queue on QP %d of %s (QP state %d)\n",
+				stuck_qp->qp_num, stuck_qp->device->name,
+				qp_attr.qp_state);
+
+		mad_agent_priv->qp_info->send_queue_stalled = 1;
+		stalled = 1;
+	}
+	return stalled;
+}
+
 static void cancel_mads(struct ib_mad_agent_private *mad_agent_priv)
 {
 	unsigned long flags;
@@ -2616,8 +2587,17 @@
 			mad_send_wr->status = IB_WC_WR_FLUSH_ERR;
 			mad_send_wr->refcount -= (mad_send_wr->timeout > 0);
 		}
+#if 0
+		list_del(&mad_send_wr->agent_list);
+		mad_agent_priv->agent.send_handler(&mad_agent_priv->agent,
+						   &mad_send_wc);
+		deref_mad_agent(mad_agent_priv);
+#endif
 	}
 
+	if (check_send_queue_stall(mad_agent_priv))
+		pr_crit("Detected stalled send queue while trying to cancel MAD's\n");
+
 	/* Empty wait list to prevent receives from finding a request */
 	list_splice_init(&mad_agent_priv->wait_list, &cancel_list);
 	spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
@@ -2708,14 +2688,10 @@
 	int free_mad;
 	struct ib_wc wc;
 	struct ib_mad_send_wc mad_send_wc;
-	bool opa;
 
 	mad_agent_priv =
 		container_of(work, struct ib_mad_agent_private, local_work);
 
-	opa = rdma_cap_opa_mad(mad_agent_priv->qp_info->port_priv->device,
-			       mad_agent_priv->qp_info->port_priv->port_num);
-
 	spin_lock_irqsave(&mad_agent_priv->lock, flags);
 	while (!list_empty(&mad_agent_priv->local_list)) {
 		local = list_entry(mad_agent_priv->local_list.next,
@@ -2728,8 +2704,7 @@
 			u8 base_version;
 			recv_mad_agent = local->recv_mad_agent;
 			if (!recv_mad_agent) {
-				dev_err(&mad_agent_priv->agent.device->dev,
-					"No receive MAD agent for local completion\n");
+				printk(KERN_ERR PFX "No receive MAD agent for local completion\n");
 				free_mad = 1;
 				goto local_send_completion;
 			}
@@ -2741,26 +2716,24 @@
 			build_smp_wc(recv_mad_agent->agent.qp,
 				     (unsigned long) local->mad_send_wr,
 				     be16_to_cpu(IB_LID_PERMISSIVE),
+				     /* FIXME upstream; pkey_index valid for IB ??? */
 				     local->mad_send_wr->send_wr.wr.ud.pkey_index,
 				     recv_mad_agent->agent.port_num, &wc);
 
 			local->mad_priv->header.recv_wc.wc = &wc;
 
-			base_version = ((struct ib_mad_hdr *)(local->mad_priv->mad))->base_version;
-			if (opa && base_version == OPA_MGMT_BASE_VERSION) {
+			base_version = local->mad_priv->mad.mad.mad_hdr.base_version;
+			if (base_version == OPA_MGMT_BASE_VERSION)
 				local->mad_priv->header.recv_wc.mad_len = local->return_wc_byte_len;
-				local->mad_priv->header.recv_wc.mad_seg_size = sizeof(struct opa_mad);
-			} else {
+			else
 				local->mad_priv->header.recv_wc.mad_len = sizeof(struct ib_mad);
-				local->mad_priv->header.recv_wc.mad_seg_size = sizeof(struct ib_mad);
-			}
 
 			INIT_LIST_HEAD(&local->mad_priv->header.recv_wc.rmpp_list);
 			list_add(&local->mad_priv->header.recv_wc.recv_buf.list,
 				 &local->mad_priv->header.recv_wc.rmpp_list);
 			local->mad_priv->header.recv_wc.recv_buf.grh = NULL;
 			local->mad_priv->header.recv_wc.recv_buf.mad =
-						(struct ib_mad *)local->mad_priv->mad;
+						&local->mad_priv->mad.mad;
 			if (atomic_read(&recv_mad_agent->qp_info->snoop_count))
 				snoop_recv(recv_mad_agent->qp_info,
 					  &local->mad_priv->header.recv_wc,
@@ -2787,8 +2760,13 @@
 
 		spin_lock_irqsave(&mad_agent_priv->lock, flags);
 		atomic_dec(&mad_agent_priv->refcount);
-		if (free_mad)
-			kfree(local->mad_priv);
+		if (free_mad) {
+			if (local->mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA)
+				kmem_cache_free(opa_mad_cache, local->mad_priv);
+			else
+				kmem_cache_free(ib_mad_cache, local->mad_priv);
+		}
+
 		kfree(local);
 	}
 	spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
@@ -2875,6 +2853,8 @@
 		atomic_dec(&mad_agent_priv->refcount);
 		spin_lock_irqsave(&mad_agent_priv->lock, flags);
 	}
+
+	check_send_queue_stall(mad_agent_priv);
 	spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
 }
 
@@ -2903,6 +2883,7 @@
 	struct ib_mad_queue *recv_queue = &qp_info->recv_queue;
 
 	/* Initialize common scatter list fields */
+	sg_list.length = sizeof *mad_priv - sizeof mad_priv->header;
 	sg_list.lkey = (*qp_info->port_priv->mr).lkey;
 
 	/* Initialize common receive WR fields */
@@ -2916,19 +2897,18 @@
 			mad_priv = mad;
 			mad = NULL;
 		} else {
-			mad_priv = alloc_mad_private(port_mad_size(qp_info->port_priv),
-						     GFP_ATOMIC);
+			mad_priv = kmem_cache_alloc(ib_mad_cache, GFP_KERNEL);
 			if (!mad_priv) {
-				dev_err(&qp_info->port_priv->device->dev,
-					"No memory for receive buffer\n");
+				printk(KERN_ERR PFX "No memory for receive buffer\n");
 				ret = -ENOMEM;
 				break;
 			}
+			mad_priv->header.flags = 0;
 		}
-		sg_list.length = mad_priv_dma_size(mad_priv);
 		sg_list.addr = ib_dma_map_single(qp_info->port_priv->device,
 						 &mad_priv->grh,
-						 mad_priv_dma_size(mad_priv),
+						 sizeof *mad_priv -
+						   sizeof mad_priv->header,
 						 DMA_FROM_DEVICE);
 		if (unlikely(ib_dma_mapping_error(qp_info->port_priv->device,
 						  sg_list.addr))) {
@@ -2952,11 +2932,12 @@
 			spin_unlock_irqrestore(&recv_queue->lock, flags);
 			ib_dma_unmap_single(qp_info->port_priv->device,
 					    mad_priv->header.mapping,
-					    mad_priv_dma_size(mad_priv),
+					    sizeof *mad_priv -
+					      sizeof mad_priv->header,
 					    DMA_FROM_DEVICE);
-			kfree(mad_priv);
-			dev_err(&qp_info->port_priv->device->dev,
-				"ib_post_recv failed: %d\n", ret);
+			BUG_ON(mad_priv->header.flags & IB_MAD_PRIV_FLAG_OPA);
+			kmem_cache_free(ib_mad_cache, mad_priv);
+			printk(KERN_ERR PFX "ib_post_recv failed: %d\n", ret);
 			break;
 		}
 	} while (post);
@@ -2989,11 +2970,21 @@
 		/* Remove from posted receive MAD list */
 		list_del(&mad_list->list);
 
-		ib_dma_unmap_single(qp_info->port_priv->device,
-				    recv->header.mapping,
-				    mad_priv_dma_size(recv),
-				    DMA_FROM_DEVICE);
-		kfree(recv);
+		if (recv->header.flags & IB_MAD_PRIV_FLAG_OPA) {
+			ib_dma_unmap_single(qp_info->port_priv->device,
+					    recv->header.mapping,
+					    sizeof(struct opa_mad_private) -
+					      sizeof(struct ib_mad_private_header),
+					    DMA_FROM_DEVICE);
+			kmem_cache_free(opa_mad_cache, recv);
+		} else {
+			ib_dma_unmap_single(qp_info->port_priv->device,
+					    recv->header.mapping,
+					    sizeof(struct ib_mad_private) -
+					      sizeof(struct ib_mad_private_header),
+					    DMA_FROM_DEVICE);
+			kmem_cache_free(ib_mad_cache, recv);
+		}
 	}
 
 	qp_info->recv_queue.count = 0;
@@ -3011,8 +3002,7 @@
 
 	attr = kmalloc(sizeof *attr, GFP_KERNEL);
 	if (!attr) {
-		dev_err(&port_priv->device->dev,
-			"Couldn't kmalloc ib_qp_attr\n");
+		printk(KERN_ERR PFX "Couldn't kmalloc ib_qp_attr\n");
 		return -ENOMEM;
 	}
 
@@ -3036,18 +3026,16 @@
 		ret = ib_modify_qp(qp, attr, IB_QP_STATE |
 					     IB_QP_PKEY_INDEX | IB_QP_QKEY);
 		if (ret) {
-			dev_err(&port_priv->device->dev,
-				"Couldn't change QP%d state to INIT: %d\n",
-				i, ret);
+			printk(KERN_ERR PFX "Couldn't change QP%d state to "
+			       "INIT: %d\n", i, ret);
 			goto out;
 		}
 
 		attr->qp_state = IB_QPS_RTR;
 		ret = ib_modify_qp(qp, attr, IB_QP_STATE);
 		if (ret) {
-			dev_err(&port_priv->device->dev,
-				"Couldn't change QP%d state to RTR: %d\n",
-				i, ret);
+			printk(KERN_ERR PFX "Couldn't change QP%d state to "
+			       "RTR: %d\n", i, ret);
 			goto out;
 		}
 
@@ -3055,18 +3043,16 @@
 		attr->sq_psn = IB_MAD_SEND_Q_PSN;
 		ret = ib_modify_qp(qp, attr, IB_QP_STATE | IB_QP_SQ_PSN);
 		if (ret) {
-			dev_err(&port_priv->device->dev,
-				"Couldn't change QP%d state to RTS: %d\n",
-				i, ret);
+			printk(KERN_ERR PFX "Couldn't change QP%d state to "
+			       "RTS: %d\n", i, ret);
 			goto out;
 		}
 	}
 
 	ret = ib_req_notify_cq(port_priv->cq, IB_CQ_NEXT_COMP);
 	if (ret) {
-		dev_err(&port_priv->device->dev,
-			"Failed to request completion notification: %d\n",
-			ret);
+		printk(KERN_ERR PFX "Failed to request completion "
+		       "notification: %d\n", ret);
 		goto out;
 	}
 
@@ -3074,10 +3060,12 @@
 		if (!port_priv->qp_info[i].qp)
 			continue;
 
-		ret = ib_mad_post_receive_mads(&port_priv->qp_info[i], NULL);
+		if (port_priv->qp_info[i].supports_opa_mads)
+			ret = ib_mad_post_opa_rcv_mads(&port_priv->qp_info[i], NULL);
+		else
+			ret = ib_mad_post_receive_mads(&port_priv->qp_info[i], NULL);
 		if (ret) {
-			dev_err(&port_priv->device->dev,
-				"Couldn't post receive WRs\n");
+			printk(KERN_ERR PFX "Couldn't post receive WRs\n");
 			goto out;
 		}
 	}
@@ -3091,8 +3079,7 @@
 	struct ib_mad_qp_info	*qp_info = qp_context;
 
 	/* It's worse than that! He's dead, Jim! */
-	dev_err(&qp_info->port_priv->device->dev,
-		"Fatal error (%d) on MAD QP (%d)\n",
+	printk(KERN_ERR PFX "Fatal error (%d) on MAD QP (%d)\n",
 		event->event, qp_info->qp->qp_num);
 }
 
@@ -3119,7 +3106,7 @@
 }
 
 static int create_mad_qp(struct ib_mad_qp_info *qp_info,
-			 enum ib_qp_type qp_type)
+			 enum ib_qp_type qp_type, int supports_opa_mads)
 {
 	struct ib_qp_init_attr	qp_init_attr;
 	int ret;
@@ -3138,15 +3125,15 @@
 	qp_init_attr.event_handler = qp_event_handler;
 	qp_info->qp = ib_create_qp(qp_info->port_priv->pd, &qp_init_attr);
 	if (IS_ERR(qp_info->qp)) {
-		dev_err(&qp_info->port_priv->device->dev,
-			"Couldn't create ib_mad QP%d\n",
-			get_spl_qp_index(qp_type));
+		printk(KERN_ERR PFX "Couldn't create ib_mad QP%d\n",
+		       get_spl_qp_index(qp_type));
 		ret = PTR_ERR(qp_info->qp);
 		goto error;
 	}
 	/* Use minimum queue sizes unless the CQ is resized */
 	qp_info->send_queue.max_active = mad_sendq_size;
 	qp_info->recv_queue.max_active = mad_recvq_size;
+	qp_info->supports_opa_mads = supports_opa_mads;
 	return 0;
 
 error:
@@ -3162,6 +3149,19 @@
 	kfree(qp_info->snoop_table);
 }
 
+static int
+mad_device_supports_opa_mads(struct ib_device *device)
+{
+	if (mad_support_opa) {
+		struct ib_device_attr attr;
+		if (!ib_query_device(device, &attr))
+			return ((attr.device_cap_flags
+				& IB_DEVICE_OPA_MAD_SUPPORT)
+				== IB_DEVICE_OPA_MAD_SUPPORT);
+	}
+	return (0);
+}
+
 /*
  * Open the port
  * Create the QP, PD, MR, and CQ if needed
@@ -3174,19 +3174,11 @@
 	unsigned long flags;
 	char name[sizeof "ib_mad123"];
 	int has_smi;
-	struct ib_cq_init_attr cq_attr = {};
-
-	if (WARN_ON(rdma_max_mad_size(device, port_num) < IB_MGMT_MAD_SIZE))
-		return -EFAULT;
-
-	if (WARN_ON(rdma_cap_opa_mad(device, port_num) &&
-		    rdma_max_mad_size(device, port_num) < OPA_MGMT_MAD_SIZE))
-		return -EFAULT;
 
 	/* Create new device info */
 	port_priv = kzalloc(sizeof *port_priv, GFP_KERNEL);
 	if (!port_priv) {
-		dev_err(&device->dev, "No memory for ib_mad_port_private\n");
+		printk(KERN_ERR PFX "No memory for ib_mad_port_private\n");
 		return -ENOMEM;
 	}
 
@@ -3198,40 +3190,46 @@
 	init_mad_qp(port_priv, &port_priv->qp_info[1]);
 
 	cq_size = mad_sendq_size + mad_recvq_size;
-	has_smi = rdma_cap_ib_smi(device, port_num);
+	has_smi = rdma_port_get_link_layer(device, port_num) == IB_LINK_LAYER_INFINIBAND;
 	if (has_smi)
 		cq_size *= 2;
 
-	cq_attr.cqe = cq_size;
 	port_priv->cq = ib_create_cq(port_priv->device,
 				     ib_mad_thread_completion_handler,
-				     NULL, port_priv, &cq_attr);
+				     NULL, port_priv, cq_size, 0);
 	if (IS_ERR(port_priv->cq)) {
-		dev_err(&device->dev, "Couldn't create ib_mad CQ\n");
+		printk(KERN_ERR PFX "Couldn't create ib_mad CQ\n");
 		ret = PTR_ERR(port_priv->cq);
 		goto error3;
 	}
 
 	port_priv->pd = ib_alloc_pd(device);
 	if (IS_ERR(port_priv->pd)) {
-		dev_err(&device->dev, "Couldn't create ib_mad PD\n");
+		printk(KERN_ERR PFX "Couldn't create ib_mad PD\n");
 		ret = PTR_ERR(port_priv->pd);
 		goto error4;
 	}
 
 	port_priv->mr = ib_get_dma_mr(port_priv->pd, IB_ACCESS_LOCAL_WRITE);
 	if (IS_ERR(port_priv->mr)) {
-		dev_err(&device->dev, "Couldn't get ib_mad DMA MR\n");
+		printk(KERN_ERR PFX "Couldn't get ib_mad DMA MR\n");
 		ret = PTR_ERR(port_priv->mr);
 		goto error5;
 	}
 
+	port_priv->supports_opa_mads = mad_device_supports_opa_mads(device);
+	if (port_priv->supports_opa_mads)
+		pr_info("Opa MAD support enabled for %s:%d\n",
+				device->name, port_num);
+
 	if (has_smi) {
-		ret = create_mad_qp(&port_priv->qp_info[0], IB_QPT_SMI);
+		ret = create_mad_qp(&port_priv->qp_info[0], IB_QPT_SMI,
+				    port_priv->supports_opa_mads);
 		if (ret)
 			goto error6;
 	}
-	ret = create_mad_qp(&port_priv->qp_info[1], IB_QPT_GSI);
+	ret = create_mad_qp(&port_priv->qp_info[1], IB_QPT_GSI,
+			    port_priv->supports_opa_mads);
 	if (ret)
 		goto error7;
 
@@ -3250,7 +3248,7 @@
 
 	ret = ib_mad_port_start(port_priv);
 	if (ret) {
-		dev_err(&device->dev, "Couldn't start port\n");
+		printk(KERN_ERR PFX "Couldn't start port\n");
 		goto error9;
 	}
 
@@ -3294,7 +3292,7 @@
 	port_priv = __ib_get_mad_port(device, port_num);
 	if (port_priv == NULL) {
 		spin_unlock_irqrestore(&ib_mad_port_list_lock, flags);
-		dev_err(&device->dev, "Port %d not found\n", port_num);
+		printk(KERN_ERR PFX "Port %d not found\n", port_num);
 		return -ENODEV;
 	}
 	list_del_init(&port_priv->port_list);
@@ -3317,21 +3315,29 @@
 
 static void ib_mad_init_device(struct ib_device *device)
 {
-	int start, i;
+	int start, end, i;
 
-	start = rdma_start_port(device);
+	if (rdma_node_get_transport(device->node_type) != RDMA_TRANSPORT_IB)
+		return;
 
-	for (i = start; i <= rdma_end_port(device); i++) {
-		if (!rdma_cap_ib_mad(device, i))
-			continue;
+	if (device->node_type == RDMA_NODE_IB_SWITCH) {
+		start = 0;
+		end   = 0;
+	} else {
+		start = 1;
+		end   = device->phys_port_cnt;
+	}
 
+	for (i = start; i <= end; i++) {
 		if (ib_mad_port_open(device, i)) {
-			dev_err(&device->dev, "Couldn't open port %d\n", i);
+			printk(KERN_ERR PFX "Couldn't open %s port %d\n",
+			       device->name, i);
 			goto error;
 		}
 		if (ib_agent_port_open(device, i)) {
-			dev_err(&device->dev,
-				"Couldn't open port %d for agents\n", i);
+			printk(KERN_ERR PFX "Couldn't open %s port %d "
+			       "for agents\n",
+			       device->name, i);
 			goto error_agent;
 		}
 	}
@@ -3339,34 +3345,46 @@
 
 error_agent:
 	if (ib_mad_port_close(device, i))
-		dev_err(&device->dev, "Couldn't close port %d\n", i);
+		printk(KERN_ERR PFX "Couldn't close %s port %d\n",
+		       device->name, i);
 
 error:
-	while (--i >= start) {
-		if (!rdma_cap_ib_mad(device, i))
-			continue;
+	i--;
 
+	while (i >= start) {
 		if (ib_agent_port_close(device, i))
-			dev_err(&device->dev,
-				"Couldn't close port %d for agents\n", i);
+			printk(KERN_ERR PFX "Couldn't close %s port %d "
+			       "for agents\n",
+			       device->name, i);
 		if (ib_mad_port_close(device, i))
-			dev_err(&device->dev, "Couldn't close port %d\n", i);
+			printk(KERN_ERR PFX "Couldn't close %s port %d\n",
+			       device->name, i);
+		i--;
 	}
 }
 
 static void ib_mad_remove_device(struct ib_device *device)
 {
-	int i;
+	int i, num_ports, cur_port;
 
-	for (i = rdma_start_port(device); i <= rdma_end_port(device); i++) {
-		if (!rdma_cap_ib_mad(device, i))
-			continue;
+	if (rdma_node_get_transport(device->node_type) != RDMA_TRANSPORT_IB)
+		return;
 
-		if (ib_agent_port_close(device, i))
-			dev_err(&device->dev,
-				"Couldn't close port %d for agents\n", i);
-		if (ib_mad_port_close(device, i))
-			dev_err(&device->dev, "Couldn't close port %d\n", i);
+	if (device->node_type == RDMA_NODE_IB_SWITCH) {
+		num_ports = 1;
+		cur_port = 0;
+	} else {
+		num_ports = device->phys_port_cnt;
+		cur_port = 1;
+	}
+	for (i = 0; i < num_ports; i++, cur_port++) {
+		if (ib_agent_port_close(device, cur_port))
+			printk(KERN_ERR PFX "Couldn't close %s port %d "
+			       "for agents\n",
+			       device->name, cur_port);
+		if (ib_mad_port_close(device, cur_port))
+			printk(KERN_ERR PFX "Couldn't close %s port %d\n",
+			       device->name, cur_port);
 	}
 }
 
@@ -3378,25 +3396,67 @@
 
 static int __init ib_mad_init_module(void)
 {
+	int ret;
+
 	mad_recvq_size = min(mad_recvq_size, IB_MAD_QP_MAX_SIZE);
 	mad_recvq_size = max(mad_recvq_size, IB_MAD_QP_MIN_SIZE);
 
 	mad_sendq_size = min(mad_sendq_size, IB_MAD_QP_MAX_SIZE);
 	mad_sendq_size = max(mad_sendq_size, IB_MAD_QP_MIN_SIZE);
 
+	ib_mad_cache = kmem_cache_create("ib_mad",
+					 sizeof(struct ib_mad_private),
+					 0,
+					 SLAB_DEBUG_FREE |
+					 SLAB_RED_ZONE |
+					 SLAB_POISON |
+					 SLAB_STORE_USER |
+					 SLAB_HWCACHE_ALIGN,
+					 NULL);
+	if (!ib_mad_cache) {
+		printk(KERN_ERR PFX "Couldn't create ib_mad cache\n");
+		ret = -ENOMEM;
+		goto error1;
+	}
+
+	opa_mad_cache = kmem_cache_create("ib_mad_opa",
+					 sizeof(struct opa_mad_private),
+					 0,
+					 SLAB_DEBUG_FREE |
+					 SLAB_RED_ZONE |
+					 SLAB_POISON |
+					 SLAB_STORE_USER |
+					 SLAB_HWCACHE_ALIGN,
+					 NULL);
+	if (!opa_mad_cache) {
+		pr_err("Couldn't create ib_mad cache\n");
+		ret = -ENOMEM;
+		goto error2;
+	}
+
 	INIT_LIST_HEAD(&ib_mad_port_list);
 
 	if (ib_register_client(&mad_client)) {
-		pr_err("Couldn't register ib_mad client\n");
-		return -EINVAL;
+		printk(KERN_ERR PFX "Couldn't register ib_mad client\n");
+		ret = -EINVAL;
+		goto error3;
 	}
 
 	return 0;
+
+error3:
+	kmem_cache_destroy(opa_mad_cache);
+error2:
+	kmem_cache_destroy(ib_mad_cache);
+error1:
+	return ret;
 }
 
 static void __exit ib_mad_cleanup_module(void)
 {
 	ib_unregister_client(&mad_client);
+	kmem_cache_destroy(opa_mad_cache);
+	kmem_cache_destroy(ib_mad_cache);
 }
 
 module_init(ib_mad_init_module);
--- a/ib_mad/mad_priv.h
+++ b/ib_mad/mad_priv.h
@@ -43,6 +43,8 @@
 #include <rdma/ib_smi.h>
 #include <rdma/opa_smi.h>
 
+#define PFX "ib_mad: "
+
 #define IB_MAD_QPS_CORE		2 /* Always QP0 and QP1 as a minimum */
 
 /* QP and CQ parameters */
@@ -57,28 +59,61 @@
 
 /* Registration table sizes */
 #define MAX_MGMT_CLASS		80
-#define MAX_MGMT_VERSION	0x83
+#define MAX_MGMT_VERSION	0x83 /* FIXME -- STL specific
+					how big should this be for STL?
+					This makes the version table huge
+					for agent registrations */
 #define MAX_MGMT_OUI		8
 #define MAX_MGMT_VENDOR_RANGE2	(IB_MGMT_CLASS_VENDOR_RANGE2_END - \
 				IB_MGMT_CLASS_VENDOR_RANGE2_START + 1)
 
+/* shared betwee mad.c and opa_mad.c */
+extern struct kmem_cache *opa_mad_cache;
+
 struct ib_mad_list_head {
 	struct list_head list;
 	struct ib_mad_queue *mad_queue;
 };
 
+enum ib_mad_private_flags {
+	IB_MAD_PRIV_FLAG_OPA = (1 << 0)
+};
 struct ib_mad_private_header {
 	struct ib_mad_list_head mad_list;
 	struct ib_mad_recv_wc recv_wc;
 	struct ib_wc wc;
 	u64 mapping;
+	u64 flags;
 } __attribute__ ((packed));
 
 struct ib_mad_private {
 	struct ib_mad_private_header header;
-	size_t mad_size;
 	struct ib_grh grh;
-	u8 mad[0];
+	union {
+		struct ib_mad mad;
+		struct ib_rmpp_mad rmpp_mad;
+		struct ib_smp smp;
+	} mad;
+} __attribute__ ((packed));
+
+/**
+ * It might be possible to define this as part of the ib_mad_private by simply
+ * extending the union there.
+ * The problem is I don't know what other RDMA hardware will do if you attempt
+ * to post >256B Rcv WR on their managment QP's.
+ * Therefore we are going to try and keep this structure separate and only use
+ * it on opa_mad capable devices.
+ * Furthermore, this allows us to use smaller kmem_cache's on non-opa capable
+ * devices for less memory ussage.
+ */
+struct opa_mad_private {
+	struct ib_mad_private_header header;
+	struct ib_grh grh;
+	union {
+		struct opa_mad mad;
+		struct opa_rmpp_mad rmpp_mad;
+		struct opa_smp smp;
+	} mad;
 } __attribute__ ((packed));
 
 struct ib_rmpp_segment {
@@ -127,6 +162,7 @@
 	struct ib_sge sg_list[IB_MAD_SEND_REQ_MAX_SG];
 	__be64 tid;
 	unsigned long timeout;
+	unsigned long sq_timeout;
 	int max_retries;
 	int retries_left;
 	int retry;
@@ -145,7 +181,7 @@
 
 struct ib_mad_local_private {
 	struct list_head completion_list;
-	struct ib_mad_private *mad_priv;
+	struct ib_mad_private *mad_priv; /* can be opa_mad_private */
 	struct ib_mad_agent_private *recv_mad_agent;
 	struct ib_mad_send_wr_private *mad_send_wr;
 	size_t return_wc_byte_len;
@@ -191,6 +227,8 @@
 	struct ib_mad_snoop_private **snoop_table;
 	int snoop_table_size;
 	atomic_t snoop_count;
+	int supports_opa_mads;
+	int send_queue_stalled;
 };
 
 struct ib_mad_port_private {
@@ -202,18 +240,20 @@
 	struct ib_mr *mr;
 
 	spinlock_t reg_lock;
+	/* FIXME with increased version range this table is huge */
 	struct ib_mad_mgmt_version_table version[MAX_MGMT_VERSION];
 	struct list_head agent_list;
 	struct workqueue_struct *wq;
 	struct work_struct work;
 	struct ib_mad_qp_info qp_info[IB_MAD_QPS_CORE];
+	int    supports_opa_mads;
 };
 
 int ib_send_mad(struct ib_mad_send_wr_private *mad_send_wr);
 
 struct ib_mad_send_wr_private *
-ib_find_send_mad(const struct ib_mad_agent_private *mad_agent_priv,
-		 const struct ib_mad_recv_wc *mad_recv_wc);
+ib_find_send_mad(struct ib_mad_agent_private *mad_agent_priv,
+		 struct ib_mad_recv_wc *mad_recv_wc);
 
 void ib_mad_complete_send_wr(struct ib_mad_send_wr_private *mad_send_wr,
 			     struct ib_mad_send_wc *mad_send_wc);
@@ -223,4 +263,28 @@
 void ib_reset_mad_timeout(struct ib_mad_send_wr_private *mad_send_wr,
 			  int timeout_ms);
 
+static inline void deref_mad_agent(struct ib_mad_agent_private *mad_agent_priv)
+{
+	if (atomic_dec_and_test(&mad_agent_priv->refcount))
+		complete(&mad_agent_priv->comp);
+}
+
+
+void dequeue_mad(struct ib_mad_list_head *mad_list);
+struct ib_mad_agent_private * find_mad_agent(
+				struct ib_mad_port_private *port_priv,
+				struct ib_mad *mad);
+
+void snoop_recv(struct ib_mad_qp_info *qp_info,
+		struct ib_mad_recv_wc *mad_recv_wc,
+		int mad_snoop_flags);
+void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr);
+void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv);
+enum smi_action handle_ib_smi(struct ib_mad_port_private *port_priv,
+			      struct ib_mad_qp_info *qp_info,
+			      struct ib_wc *wc,
+			      int port_num,
+			      struct ib_mad_private *recv,
+			      struct ib_mad_private *response);
+
 #endif	/* __IB_MAD_PRIV_H__ */
--- a/ib_mad/mad_rmpp.c
+++ b/ib_mad/mad_rmpp.c
@@ -1,7 +1,6 @@
 /*
  * Copyright (c) 2005 Intel Inc. All rights reserved.
  * Copyright (c) 2005-2006 Voltaire, Inc. All rights reserved.
- * Copyright (c) 2014 Intel Corporation.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -36,48 +35,9 @@
 
 #include "mad_priv.h"
 #include "mad_rmpp.h"
+#include "opa_mad.h"
 
-enum rmpp_state {
-	RMPP_STATE_ACTIVE,
-	RMPP_STATE_TIMEOUT,
-	RMPP_STATE_COMPLETE,
-	RMPP_STATE_CANCELING
-};
-
-struct mad_rmpp_recv {
-	struct ib_mad_agent_private *agent;
-	struct list_head list;
-	struct delayed_work timeout_work;
-	struct delayed_work cleanup_work;
-	struct completion comp;
-	enum rmpp_state state;
-	spinlock_t lock;
-	atomic_t refcount;
-
-	struct ib_ah *ah;
-	struct ib_mad_recv_wc *rmpp_wc;
-	struct ib_mad_recv_buf *cur_seg_buf;
-	int last_ack;
-	int seg_num;
-	int newwin;
-	int repwin;
-
-	__be64 tid;
-	u32 src_qp;
-	u16 slid;
-	u8 mgmt_class;
-	u8 class_version;
-	u8 method;
-	u8 base_version;
-};
-
-static inline void deref_rmpp_recv(struct mad_rmpp_recv *rmpp_recv)
-{
-	if (atomic_dec_and_test(&rmpp_recv->refcount))
-		complete(&rmpp_recv->comp);
-}
-
-static void destroy_rmpp_recv(struct mad_rmpp_recv *rmpp_recv)
+void destroy_rmpp_recv(struct mad_rmpp_recv *rmpp_recv)
 {
 	deref_rmpp_recv(rmpp_recv);
 	wait_for_completion(&rmpp_recv->comp);
@@ -112,11 +72,11 @@
 	}
 }
 
-static void format_ack(struct ib_mad_send_buf *msg,
-		       struct ib_rmpp_mad *data,
+void format_ack(struct ib_mad_send_buf *msg,
+		       struct ib_rmpp_base *data,
 		       struct mad_rmpp_recv *rmpp_recv)
 {
-	struct ib_rmpp_mad *ack = msg->mad;
+	struct ib_rmpp_base *ack = msg->mad;
 	unsigned long flags;
 
 	memcpy(ack, &data->mad_hdr, msg->hdr_len);
@@ -132,8 +92,8 @@
 	spin_unlock_irqrestore(&rmpp_recv->lock, flags);
 }
 
-static void ack_recv(struct mad_rmpp_recv *rmpp_recv,
-		     struct ib_mad_recv_wc *recv_wc)
+void ack_recv(struct mad_rmpp_recv *rmpp_recv,
+	      struct ib_mad_recv_wc *recv_wc)
 {
 	struct ib_mad_send_buf *msg;
 	int ret, hdr_len;
@@ -141,12 +101,11 @@
 	hdr_len = ib_get_mad_data_offset(recv_wc->recv_buf.mad->mad_hdr.mgmt_class);
 	msg = ib_create_send_mad(&rmpp_recv->agent->agent, recv_wc->wc->src_qp,
 				 recv_wc->wc->pkey_index, 1, hdr_len,
-				 0, GFP_KERNEL,
-				 IB_MGMT_BASE_VERSION);
+				 0, GFP_KERNEL);
 	if (IS_ERR(msg))
 		return;
 
-	format_ack(msg, (struct ib_rmpp_mad *) recv_wc->recv_buf.mad, rmpp_recv);
+	format_ack(msg, (struct ib_rmpp_base *) recv_wc->recv_buf.mad, rmpp_recv);
 	msg->ah = rmpp_recv->ah;
 	ret = ib_post_send_mad(msg, NULL);
 	if (ret)
@@ -168,8 +127,7 @@
 	hdr_len = ib_get_mad_data_offset(recv_wc->recv_buf.mad->mad_hdr.mgmt_class);
 	msg = ib_create_send_mad(agent, recv_wc->wc->src_qp,
 				 recv_wc->wc->pkey_index, 1,
-				 hdr_len, 0, GFP_KERNEL,
-				 IB_MGMT_BASE_VERSION);
+				 hdr_len, 0, GFP_KERNEL);
 	if (IS_ERR(msg))
 		ib_destroy_ah(ah);
 	else {
@@ -184,20 +142,20 @@
 		       struct ib_mad_recv_wc *recv_wc)
 {
 	struct ib_mad_send_buf *msg;
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 	int ret;
 
 	msg = alloc_response_msg(&agent->agent, recv_wc);
 	if (IS_ERR(msg))
 		return;
 
-	rmpp_mad = msg->mad;
-	memcpy(rmpp_mad, recv_wc->recv_buf.mad, msg->hdr_len);
+	rmpp_base = msg->mad;
+	memcpy(rmpp_base, recv_wc->recv_buf.mad, msg->hdr_len);
 
-	rmpp_mad->mad_hdr.method ^= IB_MGMT_METHOD_RESP;
-	ib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
-	rmpp_mad->rmpp_hdr.seg_num = 0;
-	rmpp_mad->rmpp_hdr.paylen_newwin = cpu_to_be32(1);
+	rmpp_base->mad_hdr.method ^= IB_MGMT_METHOD_RESP;
+	ib_set_rmpp_flags(&rmpp_base->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
+	rmpp_base->rmpp_hdr.seg_num = 0;
+	rmpp_base->rmpp_hdr.paylen_newwin = cpu_to_be32(1);
 
 	ret = ib_post_send_mad(msg, NULL);
 	if (ret) {
@@ -213,27 +171,27 @@
 	ib_free_send_mad(mad_send_wc->send_buf);
 }
 
-static void nack_recv(struct ib_mad_agent_private *agent,
-		      struct ib_mad_recv_wc *recv_wc, u8 rmpp_status)
+void nack_recv(struct ib_mad_agent_private *agent,
+	       struct ib_mad_recv_wc *recv_wc, u8 rmpp_status)
 {
 	struct ib_mad_send_buf *msg;
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 	int ret;
 
 	msg = alloc_response_msg(&agent->agent, recv_wc);
 	if (IS_ERR(msg))
 		return;
 
-	rmpp_mad = msg->mad;
-	memcpy(rmpp_mad, recv_wc->recv_buf.mad, msg->hdr_len);
+	rmpp_base = msg->mad;
+	memcpy(rmpp_base, recv_wc->recv_buf.mad, msg->hdr_len);
 
-	rmpp_mad->mad_hdr.method ^= IB_MGMT_METHOD_RESP;
-	rmpp_mad->rmpp_hdr.rmpp_version = IB_MGMT_RMPP_VERSION;
-	rmpp_mad->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_ABORT;
-	ib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
-	rmpp_mad->rmpp_hdr.rmpp_status = rmpp_status;
-	rmpp_mad->rmpp_hdr.seg_num = 0;
-	rmpp_mad->rmpp_hdr.paylen_newwin = 0;
+	rmpp_base->mad_hdr.method ^= IB_MGMT_METHOD_RESP;
+	rmpp_base->rmpp_hdr.rmpp_version = IB_MGMT_RMPP_VERSION;
+	rmpp_base->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_ABORT;
+	ib_set_rmpp_flags(&rmpp_base->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
+	rmpp_base->rmpp_hdr.rmpp_status = rmpp_status;
+	rmpp_base->rmpp_hdr.seg_num = 0;
+	rmpp_base->rmpp_hdr.paylen_newwin = 0;
 
 	ret = ib_post_send_mad(msg, NULL);
 	if (ret) {
@@ -264,7 +222,7 @@
 	ib_free_recv_mad(rmpp_wc);
 }
 
-static void recv_cleanup_handler(struct work_struct *work)
+void recv_cleanup_handler(struct work_struct *work)
 {
 	struct mad_rmpp_recv *rmpp_recv =
 		container_of(work, struct mad_rmpp_recv, cleanup_work.work);
@@ -280,7 +238,7 @@
 	destroy_rmpp_recv(rmpp_recv);
 }
 
-static struct mad_rmpp_recv *
+struct mad_rmpp_recv *
 create_rmpp_recv(struct ib_mad_agent_private *agent,
 		 struct ib_mad_recv_wc *mad_recv_wc)
 {
@@ -327,7 +285,7 @@
 	return NULL;
 }
 
-static struct mad_rmpp_recv *
+struct mad_rmpp_recv *
 find_rmpp_recv(struct ib_mad_agent_private *agent,
 	       struct ib_mad_recv_wc *mad_recv_wc)
 {
@@ -346,7 +304,7 @@
 	return NULL;
 }
 
-static struct mad_rmpp_recv *
+struct mad_rmpp_recv *
 acquire_rmpp_recv(struct ib_mad_agent_private *agent,
 		  struct ib_mad_recv_wc *mad_recv_wc)
 {
@@ -361,7 +319,7 @@
 	return rmpp_recv;
 }
 
-static struct mad_rmpp_recv *
+struct mad_rmpp_recv *
 insert_rmpp_recv(struct ib_mad_agent_private *agent,
 		 struct mad_rmpp_recv *rmpp_recv)
 {
@@ -374,22 +332,6 @@
 	return cur_rmpp_recv;
 }
 
-static inline int get_last_flag(struct ib_mad_recv_buf *seg)
-{
-	struct ib_rmpp_mad *rmpp_mad;
-
-	rmpp_mad = (struct ib_rmpp_mad *) seg->mad;
-	return ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) & IB_MGMT_RMPP_FLAG_LAST;
-}
-
-static inline int get_seg_num(struct ib_mad_recv_buf *seg)
-{
-	struct ib_rmpp_mad *rmpp_mad;
-
-	rmpp_mad = (struct ib_rmpp_mad *) seg->mad;
-	return be32_to_cpu(rmpp_mad->rmpp_hdr.seg_num);
-}
-
 static inline struct ib_mad_recv_buf * get_next_seg(struct list_head *rmpp_list,
 						    struct ib_mad_recv_buf *seg)
 {
@@ -420,8 +362,8 @@
 	return NULL;
 }
 
-static void update_seg_num(struct mad_rmpp_recv *rmpp_recv,
-			   struct ib_mad_recv_buf *new_buf)
+void update_seg_num(struct mad_rmpp_recv *rmpp_recv,
+		    struct ib_mad_recv_buf *new_buf)
 {
 	struct list_head *rmpp_list = &rmpp_recv->rmpp_wc->rmpp_list;
 
@@ -436,23 +378,14 @@
 {
 	struct ib_rmpp_mad *rmpp_mad;
 	int hdr_size, data_size, pad;
-	bool opa = rdma_cap_opa_mad(rmpp_recv->agent->qp_info->port_priv->device,
-				    rmpp_recv->agent->qp_info->port_priv->port_num);
 
 	rmpp_mad = (struct ib_rmpp_mad *)rmpp_recv->cur_seg_buf->mad;
 
-	hdr_size = ib_get_mad_data_offset(rmpp_mad->mad_hdr.mgmt_class);
-	if (opa && rmpp_recv->base_version == OPA_MGMT_BASE_VERSION) {
-		data_size = sizeof(struct opa_rmpp_mad) - hdr_size;
-		pad = OPA_MGMT_RMPP_DATA - be32_to_cpu(rmpp_mad->rmpp_hdr.paylen_newwin);
-		if (pad > OPA_MGMT_RMPP_DATA || pad < 0)
-			pad = 0;
-	} else {
-		data_size = sizeof(struct ib_rmpp_mad) - hdr_size;
-		pad = IB_MGMT_RMPP_DATA - be32_to_cpu(rmpp_mad->rmpp_hdr.paylen_newwin);
-		if (pad > IB_MGMT_RMPP_DATA || pad < 0)
-			pad = 0;
-	}
+	hdr_size = ib_get_mad_data_offset(rmpp_mad->base.mad_hdr.mgmt_class);
+	data_size = sizeof(struct ib_rmpp_mad) - hdr_size;
+	pad = IB_MGMT_RMPP_DATA - be32_to_cpu(rmpp_mad->base.rmpp_hdr.paylen_newwin);
+	if (pad > IB_MGMT_RMPP_DATA || pad < 0)
+		pad = 0;
 
 	return hdr_size + rmpp_recv->seg_num * data_size - pad;
 }
@@ -579,21 +512,20 @@
 	u32 paylen = 0;
 
 	rmpp_mad = mad_send_wr->send_buf.mad;
-	ib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
-	rmpp_mad->rmpp_hdr.seg_num = cpu_to_be32(++mad_send_wr->seg_num);
+	ib_set_rmpp_flags(&rmpp_mad->base.rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);
+	rmpp_mad->base.rmpp_hdr.seg_num = cpu_to_be32(++mad_send_wr->seg_num);
 
 	if (mad_send_wr->seg_num == 1) {
-		rmpp_mad->rmpp_hdr.rmpp_rtime_flags |= IB_MGMT_RMPP_FLAG_FIRST;
-		paylen = (mad_send_wr->send_buf.seg_count *
-			  mad_send_wr->send_buf.seg_rmpp_size) -
-			  mad_send_wr->pad;
+		rmpp_mad->base.rmpp_hdr.rmpp_rtime_flags |= IB_MGMT_RMPP_FLAG_FIRST;
+		paylen = mad_send_wr->send_buf.seg_count * IB_MGMT_RMPP_DATA -
+			 mad_send_wr->pad;
 	}
 
 	if (mad_send_wr->seg_num == mad_send_wr->send_buf.seg_count) {
-		rmpp_mad->rmpp_hdr.rmpp_rtime_flags |= IB_MGMT_RMPP_FLAG_LAST;
-		paylen = mad_send_wr->send_buf.seg_rmpp_size - mad_send_wr->pad;
+		rmpp_mad->base.rmpp_hdr.rmpp_rtime_flags |= IB_MGMT_RMPP_FLAG_LAST;
+		paylen = IB_MGMT_RMPP_DATA - mad_send_wr->pad;
 	}
-	rmpp_mad->rmpp_hdr.paylen_newwin = cpu_to_be32(paylen);
+	rmpp_mad->base.rmpp_hdr.paylen_newwin = cpu_to_be32(paylen);
 
 	/* 2 seconds for an ACK until we can find the packet lifetime */
 	timeout = mad_send_wr->send_buf.timeout_ms;
@@ -603,7 +535,7 @@
 	return ib_send_mad(mad_send_wr);
 }
 
-static void abort_send(struct ib_mad_agent_private *agent,
+void abort_send(struct ib_mad_agent_private *agent,
 		       struct ib_mad_recv_wc *mad_recv_wc, u8 rmpp_status)
 {
 	struct ib_mad_send_wr_private *mad_send_wr;
@@ -631,18 +563,6 @@
 	spin_unlock_irqrestore(&agent->lock, flags);
 }
 
-static inline void adjust_last_ack(struct ib_mad_send_wr_private *wr,
-				   int seg_num)
-{
-	struct list_head *list;
-
-	wr->last_ack = seg_num;
-	list = &wr->last_ack_seg->list;
-	list_for_each_entry(wr->last_ack_seg, list, list)
-		if (wr->last_ack_seg->num == seg_num)
-			break;
-}
-
 static void process_ds_ack(struct ib_mad_agent_private *agent,
 			   struct ib_mad_recv_wc *mad_recv_wc, int newwin)
 {
@@ -653,23 +573,23 @@
 		rmpp_recv->repwin = newwin;
 }
 
-static void process_rmpp_ack(struct ib_mad_agent_private *agent,
+void process_rmpp_ack(struct ib_mad_agent_private *agent,
 			     struct ib_mad_recv_wc *mad_recv_wc)
 {
 	struct ib_mad_send_wr_private *mad_send_wr;
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 	unsigned long flags;
 	int seg_num, newwin, ret;
 
-	rmpp_mad = (struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad;
-	if (rmpp_mad->rmpp_hdr.rmpp_status) {
+	rmpp_base = (struct ib_rmpp_base *)mad_recv_wc->recv_buf.mad;
+	if (rmpp_base->rmpp_hdr.rmpp_status) {
 		abort_send(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 		nack_recv(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 		return;
 	}
 
-	seg_num = be32_to_cpu(rmpp_mad->rmpp_hdr.seg_num);
-	newwin = be32_to_cpu(rmpp_mad->rmpp_hdr.paylen_newwin);
+	seg_num = be32_to_cpu(rmpp_base->rmpp_hdr.seg_num);
+	newwin = be32_to_cpu(rmpp_base->rmpp_hdr.paylen_newwin);
 	if (newwin < seg_num) {
 		abort_send(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_W2S);
 		nack_recv(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_W2S);
@@ -754,7 +674,7 @@
 	struct ib_rmpp_hdr *rmpp_hdr;
 	u8 rmpp_status;
 
-	rmpp_hdr = &((struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad)->rmpp_hdr;
+	rmpp_hdr = &((struct ib_rmpp_base *)mad_recv_wc->recv_buf.mad)->rmpp_hdr;
 
 	if (rmpp_hdr->rmpp_status) {
 		rmpp_status = IB_MGMT_RMPP_STATUS_BAD_STATUS;
@@ -780,33 +700,34 @@
 	return NULL;
 }
 
-static void process_rmpp_stop(struct ib_mad_agent_private *agent,
+void process_rmpp_stop(struct ib_mad_agent_private *agent,
 			      struct ib_mad_recv_wc *mad_recv_wc)
 {
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 
-	rmpp_mad = (struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad;
+	rmpp_base = (struct ib_rmpp_base *)mad_recv_wc->recv_buf.mad;
 
-	if (rmpp_mad->rmpp_hdr.rmpp_status != IB_MGMT_RMPP_STATUS_RESX) {
+	if (rmpp_base->rmpp_hdr.rmpp_status != IB_MGMT_RMPP_STATUS_RESX) {
 		abort_send(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 		nack_recv(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 	} else
-		abort_send(agent, mad_recv_wc, rmpp_mad->rmpp_hdr.rmpp_status);
+		abort_send(agent, mad_recv_wc, rmpp_base->rmpp_hdr.rmpp_status);
+
 }
 
-static void process_rmpp_abort(struct ib_mad_agent_private *agent,
+void process_rmpp_abort(struct ib_mad_agent_private *agent,
 			       struct ib_mad_recv_wc *mad_recv_wc)
 {
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 
-	rmpp_mad = (struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad;
+	rmpp_base = (struct ib_rmpp_base *)mad_recv_wc->recv_buf.mad;
 
-	if (rmpp_mad->rmpp_hdr.rmpp_status < IB_MGMT_RMPP_STATUS_ABORT_MIN ||
-	    rmpp_mad->rmpp_hdr.rmpp_status > IB_MGMT_RMPP_STATUS_ABORT_MAX) {
+	if (rmpp_base->rmpp_hdr.rmpp_status < IB_MGMT_RMPP_STATUS_ABORT_MIN ||
+	    rmpp_base->rmpp_hdr.rmpp_status > IB_MGMT_RMPP_STATUS_ABORT_MAX) {
 		abort_send(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 		nack_recv(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_BAD_STATUS);
 	} else
-		abort_send(agent, mad_recv_wc, rmpp_mad->rmpp_hdr.rmpp_status);
+		abort_send(agent, mad_recv_wc, rmpp_base->rmpp_hdr.rmpp_status);
 }
 
 struct ib_mad_recv_wc *
@@ -816,16 +737,16 @@
 	struct ib_rmpp_mad *rmpp_mad;
 
 	rmpp_mad = (struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad;
-	if (!(rmpp_mad->rmpp_hdr.rmpp_rtime_flags & IB_MGMT_RMPP_FLAG_ACTIVE))
+	if (!(rmpp_mad->base.rmpp_hdr.rmpp_rtime_flags & IB_MGMT_RMPP_FLAG_ACTIVE))
 		return mad_recv_wc;
 
-	if (rmpp_mad->rmpp_hdr.rmpp_version != IB_MGMT_RMPP_VERSION) {
+	if (rmpp_mad->base.rmpp_hdr.rmpp_version != IB_MGMT_RMPP_VERSION) {
 		abort_send(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_UNV);
 		nack_recv(agent, mad_recv_wc, IB_MGMT_RMPP_STATUS_UNV);
 		goto out;
 	}
 
-	switch (rmpp_mad->rmpp_hdr.rmpp_type) {
+	switch (rmpp_mad->base.rmpp_hdr.rmpp_type) {
 	case IB_MGMT_RMPP_TYPE_DATA:
 		return process_rmpp_data(agent, mad_recv_wc);
 	case IB_MGMT_RMPP_TYPE_ACK:
@@ -886,11 +807,11 @@
 	int ret;
 
 	rmpp_mad = mad_send_wr->send_buf.mad;
-	if (!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &
+	if (!(ib_get_rmpp_flags(&rmpp_mad->base.rmpp_hdr) &
 	      IB_MGMT_RMPP_FLAG_ACTIVE))
 		return IB_RMPP_RESULT_UNHANDLED;
 
-	if (rmpp_mad->rmpp_hdr.rmpp_type != IB_MGMT_RMPP_TYPE_DATA) {
+	if (rmpp_mad->base.rmpp_hdr.rmpp_type != IB_MGMT_RMPP_TYPE_DATA) {
 		mad_send_wr->seg_num = 1;
 		return IB_RMPP_RESULT_INTERNAL;
 	}
@@ -908,15 +829,15 @@
 int ib_process_rmpp_send_wc(struct ib_mad_send_wr_private *mad_send_wr,
 			    struct ib_mad_send_wc *mad_send_wc)
 {
-	struct ib_rmpp_mad *rmpp_mad;
+	struct ib_rmpp_base *rmpp_base;
 	int ret;
 
-	rmpp_mad = mad_send_wr->send_buf.mad;
-	if (!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &
+	rmpp_base = mad_send_wr->send_buf.mad;
+	if (!(ib_get_rmpp_flags(&rmpp_base->rmpp_hdr) &
 	      IB_MGMT_RMPP_FLAG_ACTIVE))
 		return IB_RMPP_RESULT_UNHANDLED; /* RMPP not active */
 
-	if (rmpp_mad->rmpp_hdr.rmpp_type != IB_MGMT_RMPP_TYPE_DATA)
+	if (rmpp_base->rmpp_hdr.rmpp_type != IB_MGMT_RMPP_TYPE_DATA)
 		return IB_RMPP_RESULT_INTERNAL;	 /* ACK, STOP, or ABORT */
 
 	if (mad_send_wc->status != IB_WC_SUCCESS ||
@@ -950,7 +871,7 @@
 	int ret;
 
 	rmpp_mad = mad_send_wr->send_buf.mad;
-	if (!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &
+	if (!(ib_get_rmpp_flags(&rmpp_mad->base.rmpp_hdr) &
 	      IB_MGMT_RMPP_FLAG_ACTIVE))
 		return IB_RMPP_RESULT_UNHANDLED; /* RMPP not active */
 
--- a/ib_mad/mad_rmpp.h
+++ b/ib_mad/mad_rmpp.h
@@ -40,6 +40,41 @@
 	IB_RMPP_RESULT_UNHANDLED
 };
 
+enum rmpp_state {
+	RMPP_STATE_ACTIVE,
+	RMPP_STATE_TIMEOUT,
+	RMPP_STATE_COMPLETE,
+	RMPP_STATE_CANCELING
+};
+
+struct mad_rmpp_recv {
+	struct ib_mad_agent_private *agent;
+	struct list_head list;
+	struct delayed_work timeout_work;
+	struct delayed_work cleanup_work;
+	struct completion comp;
+	enum rmpp_state state;
+	spinlock_t lock;
+	atomic_t refcount;
+
+	struct ib_ah *ah;
+	struct ib_mad_recv_wc *rmpp_wc;
+	struct ib_mad_recv_buf *cur_seg_buf;
+	int last_ack;
+	int seg_num;
+	int newwin;
+	int repwin;
+
+	__be64 tid;
+	u32 src_qp;
+	u16 slid;
+	u8 mgmt_class;
+	u8 class_version;
+	u8 method;
+	u8 base_version; /* indicates Jumbo or IB MAD */
+};
+
+
 int ib_send_rmpp_mad(struct ib_mad_send_wr_private *mad_send_wr);
 
 struct ib_mad_recv_wc *
@@ -55,4 +90,71 @@
 
 int ib_retry_rmpp(struct ib_mad_send_wr_private *mad_send_wr);
 
+static inline void deref_rmpp_recv(struct mad_rmpp_recv *rmpp_recv)
+{
+	if (atomic_dec_and_test(&rmpp_recv->refcount))
+		complete(&rmpp_recv->comp);
+}
+
+void destroy_rmpp_recv(struct mad_rmpp_recv *rmpp_recv);
+
+void format_ack(struct ib_mad_send_buf *msg,
+		       struct ib_rmpp_base *data,
+		       struct mad_rmpp_recv *rmpp_recv);
+
+void nack_recv(struct ib_mad_agent_private *agent,
+	       struct ib_mad_recv_wc *recv_wc, u8 rmpp_status);
+struct mad_rmpp_recv *
+find_rmpp_recv(struct ib_mad_agent_private *agent,
+	       struct ib_mad_recv_wc *mad_recv_wc);
+static inline int get_seg_num(struct ib_mad_recv_buf *seg)
+{
+	struct ib_rmpp_base *rmpp_base;
+
+	rmpp_base = (struct ib_rmpp_base *) seg->mad;
+	return be32_to_cpu(rmpp_base->rmpp_hdr.seg_num);
+}
+
+void ack_recv(struct mad_rmpp_recv *rmpp_recv,
+	      struct ib_mad_recv_wc *recv_wc);
+struct mad_rmpp_recv *
+acquire_rmpp_recv(struct ib_mad_agent_private *agent,
+		  struct ib_mad_recv_wc *mad_recv_wc);
+void update_seg_num(struct mad_rmpp_recv *rmpp_recv,
+		    struct ib_mad_recv_buf *new_buf);
+struct mad_rmpp_recv *
+insert_rmpp_recv(struct ib_mad_agent_private *agent,
+		 struct mad_rmpp_recv *rmpp_recv);
+struct mad_rmpp_recv *
+create_rmpp_recv(struct ib_mad_agent_private *agent,
+		 struct ib_mad_recv_wc *mad_recv_wc);
+static inline int get_last_flag(struct ib_mad_recv_buf *seg)
+{
+	struct ib_rmpp_base *rmpp_base;
+
+	rmpp_base = (struct ib_rmpp_base *) seg->mad;
+	return ib_get_rmpp_flags(&rmpp_base->rmpp_hdr) & IB_MGMT_RMPP_FLAG_LAST;
+}
+
+void abort_send(struct ib_mad_agent_private *agent,
+		       struct ib_mad_recv_wc *mad_recv_wc, u8 rmpp_status);
+void recv_cleanup_handler(struct work_struct *work);
+static inline void adjust_last_ack(struct ib_mad_send_wr_private *wr,
+				   int seg_num)
+{
+	struct list_head *list;
+
+	wr->last_ack = seg_num;
+	list = &wr->last_ack_seg->list;
+	list_for_each_entry(wr->last_ack_seg, list, list)
+		if (wr->last_ack_seg->num == seg_num)
+			break;
+}
+void process_rmpp_abort(struct ib_mad_agent_private *agent,
+			       struct ib_mad_recv_wc *mad_recv_wc);
+void process_rmpp_ack(struct ib_mad_agent_private *agent,
+			     struct ib_mad_recv_wc *mad_recv_wc);
+void process_rmpp_stop(struct ib_mad_agent_private *agent,
+			      struct ib_mad_recv_wc *mad_recv_wc);
+
 #endif	/* __MAD_RMPP_H__ */
--- a/ib_mad/opa_smi.h
+++ b/ib_mad/opa_smi.h
@@ -39,19 +39,19 @@
 
 #include "smi.h"
 
-enum smi_action opa_smi_handle_dr_smp_recv(struct opa_smp *smp, bool is_switch,
+enum smi_action opa_smi_handle_dr_smp_recv(struct opa_smp *smp, u8 node_type,
 				       int port_num, int phys_port_cnt);
 int opa_smi_get_fwd_port(struct opa_smp *smp);
 extern enum smi_forward_action opa_smi_check_forward_dr_smp(struct opa_smp *smp);
 extern enum smi_action opa_smi_handle_dr_smp_send(struct opa_smp *smp,
-					      bool is_switch, int port_num);
+					      u8 node_type, int port_num);
 
 /*
  * Return IB_SMI_HANDLE if the SMP should be handled by the local SMA/SM
  * via process_mad
  */
 static inline enum smi_action opa_smi_check_local_smp(struct opa_smp *smp,
-						      struct ib_device *device)
+						  struct ib_device *device)
 {
 	/* C14-9:3 -- We're at the end of the DR segment of path */
 	/* C14-9:4 -- Hop Pointer = Hop Count + 1 -> give to SMA/SM */
@@ -66,7 +66,7 @@
  * via process_mad
  */
 static inline enum smi_action opa_smi_check_local_returning_smp(struct opa_smp *smp,
-								struct ib_device *device)
+						   struct ib_device *device)
 {
 	/* C14-13:3 -- We're at the end of the DR segment of path */
 	/* C14-13:4 -- Hop Pointer == 0 -> give to SM */
--- a/ib_mad/smi.c
+++ b/ib_mad/smi.c
@@ -5,7 +5,6 @@
  * Copyright (c) 2004, 2005 Topspin Corporation.  All rights reserved.
  * Copyright (c) 2004-2007 Voltaire Corporation.  All rights reserved.
  * Copyright (c) 2005 Sun Microsystems, Inc. All rights reserved.
- * Copyright (c) 2014 Intel Corporation.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -39,82 +38,85 @@
 
 #include <rdma/ib_smi.h>
 #include "smi.h"
-#include "opa_smi.h"
 
-static enum smi_action __smi_handle_dr_smp_send(bool is_switch, int port_num,
-						u8 *hop_ptr, u8 hop_cnt,
-						const u8 *initial_path,
-						const u8 *return_path,
-						u8 direction,
-						bool dr_dlid_is_permissive,
-						bool dr_slid_is_permissive)
+/*
+ * Fixup a directed route SMP for sending
+ * Return 0 if the SMP should be discarded
+ */
+enum smi_action smi_handle_dr_smp_send(struct ib_smp *smp,
+				       u8 node_type, int port_num)
 {
+	u8 hop_ptr, hop_cnt;
+
+	hop_ptr = smp->hop_ptr;
+	hop_cnt = smp->hop_cnt;
+
 	/* See section 14.2.2.2, Vol 1 IB spec */
 	/* C14-6 -- valid hop_cnt values are from 0 to 63 */
 	if (hop_cnt >= IB_SMP_MAX_PATH_HOPS)
 		return IB_SMI_DISCARD;
 
-	if (!direction) {
+	if (!ib_get_smp_direction(smp)) {
 		/* C14-9:1 */
-		if (hop_cnt && *hop_ptr == 0) {
-			(*hop_ptr)++;
-			return (initial_path[*hop_ptr] ==
+		if (hop_cnt && hop_ptr == 0) {
+			smp->hop_ptr++;
+			return (smp->initial_path[smp->hop_ptr] ==
 				port_num ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-9:2 */
-		if (*hop_ptr && *hop_ptr < hop_cnt) {
-			if (!is_switch)
+		if (hop_ptr && hop_ptr < hop_cnt) {
+			if (node_type != RDMA_NODE_IB_SWITCH)
 				return IB_SMI_DISCARD;
 
-			/* return_path set when received */
-			(*hop_ptr)++;
-			return (initial_path[*hop_ptr] ==
+			/* smp->return_path set when received */
+			smp->hop_ptr++;
+			return (smp->initial_path[smp->hop_ptr] ==
 				port_num ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-9:3 -- We're at the end of the DR segment of path */
-		if (*hop_ptr == hop_cnt) {
-			/* return_path set when received */
-			(*hop_ptr)++;
-			return (is_switch ||
-				dr_dlid_is_permissive ?
+		if (hop_ptr == hop_cnt) {
+			/* smp->return_path set when received */
+			smp->hop_ptr++;
+			return (node_type == RDMA_NODE_IB_SWITCH ||
+				smp->dr_dlid == IB_LID_PERMISSIVE ?
 				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-9:4 -- hop_ptr = hop_cnt + 1 -> give to SMA/SM */
 		/* C14-9:5 -- Fail unreasonable hop pointer */
-		return (*hop_ptr == hop_cnt + 1 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
+		return (hop_ptr == hop_cnt + 1 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 
 	} else {
 		/* C14-13:1 */
-		if (hop_cnt && *hop_ptr == hop_cnt + 1) {
-			(*hop_ptr)--;
-			return (return_path[*hop_ptr] ==
+		if (hop_cnt && hop_ptr == hop_cnt + 1) {
+			smp->hop_ptr--;
+			return (smp->return_path[smp->hop_ptr] ==
 				port_num ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:2 */
-		if (2 <= *hop_ptr && *hop_ptr <= hop_cnt) {
-			if (!is_switch)
+		if (2 <= hop_ptr && hop_ptr <= hop_cnt) {
+			if (node_type != RDMA_NODE_IB_SWITCH)
 				return IB_SMI_DISCARD;
 
-			(*hop_ptr)--;
-			return (return_path[*hop_ptr] ==
+			smp->hop_ptr--;
+			return (smp->return_path[smp->hop_ptr] ==
 				port_num ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:3 -- at the end of the DR segment of path */
-		if (*hop_ptr == 1) {
-			(*hop_ptr)--;
+		if (hop_ptr == 1) {
+			smp->hop_ptr--;
 			/* C14-13:3 -- SMPs destined for SM shouldn't be here */
-			return (is_switch ||
-				dr_slid_is_permissive ?
+			return (node_type == RDMA_NODE_IB_SWITCH ||
+				smp->dr_slid == IB_LID_PERMISSIVE ?
 				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:4 -- hop_ptr = 0 -> should have gone to SM */
-		if (*hop_ptr == 0)
+		if (hop_ptr == 0)
 			return IB_SMI_HANDLE;
 
 		/* C14-13:5 -- Check for unreasonable hop pointer */
@@ -123,163 +125,105 @@
 }
 
 /*
- * Fixup a directed route SMP for sending
- * Return IB_SMI_DISCARD if the SMP should be discarded
+ * Adjust information for a received SMP
+ * Return 0 if the SMP should be dropped
  */
-enum smi_action smi_handle_dr_smp_send(struct ib_smp *smp,
-				       bool is_switch, int port_num)
+enum smi_action smi_handle_dr_smp_recv(struct ib_smp *smp, u8 node_type,
+				       int port_num, int phys_port_cnt)
 {
-	return __smi_handle_dr_smp_send(is_switch, port_num,
-					&smp->hop_ptr, smp->hop_cnt,
-					smp->initial_path,
-					smp->return_path,
-					ib_get_smp_direction(smp),
-					smp->dr_dlid == IB_LID_PERMISSIVE,
-					smp->dr_slid == IB_LID_PERMISSIVE);
-}
+	u8 hop_ptr, hop_cnt;
 
-enum smi_action opa_smi_handle_dr_smp_send(struct opa_smp *smp,
-				       bool is_switch, int port_num)
-{
-	return __smi_handle_dr_smp_send(is_switch, port_num,
-					&smp->hop_ptr, smp->hop_cnt,
-					smp->route.dr.initial_path,
-					smp->route.dr.return_path,
-					opa_get_smp_direction(smp),
-					smp->route.dr.dr_dlid ==
-					OPA_LID_PERMISSIVE,
-					smp->route.dr.dr_slid ==
-					OPA_LID_PERMISSIVE);
-}
+	hop_ptr = smp->hop_ptr;
+	hop_cnt = smp->hop_cnt;
 
-static enum smi_action __smi_handle_dr_smp_recv(bool is_switch, int port_num,
-						int phys_port_cnt,
-						u8 *hop_ptr, u8 hop_cnt,
-						const u8 *initial_path,
-						u8 *return_path,
-						u8 direction,
-						bool dr_dlid_is_permissive,
-						bool dr_slid_is_permissive)
-{
 	/* See section 14.2.2.2, Vol 1 IB spec */
 	/* C14-6 -- valid hop_cnt values are from 0 to 63 */
 	if (hop_cnt >= IB_SMP_MAX_PATH_HOPS)
 		return IB_SMI_DISCARD;
 
-	if (!direction) {
+	if (!ib_get_smp_direction(smp)) {
 		/* C14-9:1 -- sender should have incremented hop_ptr */
-		if (hop_cnt && *hop_ptr == 0)
+		if (hop_cnt && hop_ptr == 0)
 			return IB_SMI_DISCARD;
 
 		/* C14-9:2 -- intermediate hop */
-		if (*hop_ptr && *hop_ptr < hop_cnt) {
-			if (!is_switch)
+		if (hop_ptr && hop_ptr < hop_cnt) {
+			if (node_type != RDMA_NODE_IB_SWITCH)
 				return IB_SMI_DISCARD;
 
-			return_path[*hop_ptr] = port_num;
-			/* hop_ptr updated when sending */
-			return (initial_path[*hop_ptr+1] <= phys_port_cnt ?
+			smp->return_path[hop_ptr] = port_num;
+			/* smp->hop_ptr updated when sending */
+			return (smp->initial_path[hop_ptr+1] <= phys_port_cnt ?
 				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-9:3 -- We're at the end of the DR segment of path */
-		if (*hop_ptr == hop_cnt) {
+		if (hop_ptr == hop_cnt) {
 			if (hop_cnt)
-				return_path[*hop_ptr] = port_num;
-			/* hop_ptr updated when sending */
+				smp->return_path[hop_ptr] = port_num;
+			/* smp->hop_ptr updated when sending */
 
-			return (is_switch ||
-				dr_dlid_is_permissive ?
+			return (node_type == RDMA_NODE_IB_SWITCH ||
+				smp->dr_dlid == IB_LID_PERMISSIVE ?
 				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-9:4 -- hop_ptr = hop_cnt + 1 -> give to SMA/SM */
 		/* C14-9:5 -- fail unreasonable hop pointer */
-		return (*hop_ptr == hop_cnt + 1 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
+		return (hop_ptr == hop_cnt + 1 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 
 	} else {
 
 		/* C14-13:1 */
-		if (hop_cnt && *hop_ptr == hop_cnt + 1) {
-			(*hop_ptr)--;
-			return (return_path[*hop_ptr] ==
+		if (hop_cnt && hop_ptr == hop_cnt + 1) {
+			smp->hop_ptr--;
+			return (smp->return_path[smp->hop_ptr] ==
 				port_num ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:2 */
-		if (2 <= *hop_ptr && *hop_ptr <= hop_cnt) {
-			if (!is_switch)
+		if (2 <= hop_ptr && hop_ptr <= hop_cnt) {
+			if (node_type != RDMA_NODE_IB_SWITCH)
 				return IB_SMI_DISCARD;
 
-			/* hop_ptr updated when sending */
-			return (return_path[*hop_ptr-1] <= phys_port_cnt ?
+			/* smp->hop_ptr updated when sending */
+			return (smp->return_path[hop_ptr-1] <= phys_port_cnt ?
 				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:3 -- We're at the end of the DR segment of path */
-		if (*hop_ptr == 1) {
-			if (dr_slid_is_permissive) {
+		if (hop_ptr == 1) {
+			if (smp->dr_slid == IB_LID_PERMISSIVE) {
 				/* giving SMP to SM - update hop_ptr */
-				(*hop_ptr)--;
+				smp->hop_ptr--;
 				return IB_SMI_HANDLE;
 			}
-			/* hop_ptr updated when sending */
-			return (is_switch ? IB_SMI_HANDLE : IB_SMI_DISCARD);
+			/* smp->hop_ptr updated when sending */
+			return (node_type == RDMA_NODE_IB_SWITCH ?
+				IB_SMI_HANDLE : IB_SMI_DISCARD);
 		}
 
 		/* C14-13:4 -- hop_ptr = 0 -> give to SM */
 		/* C14-13:5 -- Check for unreasonable hop pointer */
-		return (*hop_ptr == 0 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
+		return (hop_ptr == 0 ? IB_SMI_HANDLE : IB_SMI_DISCARD);
 	}
 }
 
-/*
- * Adjust information for a received SMP
- * Return IB_SMI_DISCARD if the SMP should be dropped
- */
-enum smi_action smi_handle_dr_smp_recv(struct ib_smp *smp, bool is_switch,
-				       int port_num, int phys_port_cnt)
+enum smi_forward_action smi_check_forward_dr_smp(struct ib_smp *smp)
 {
-	return __smi_handle_dr_smp_recv(is_switch, port_num, phys_port_cnt,
-					&smp->hop_ptr, smp->hop_cnt,
-					smp->initial_path,
-					smp->return_path,
-					ib_get_smp_direction(smp),
-					smp->dr_dlid == IB_LID_PERMISSIVE,
-					smp->dr_slid == IB_LID_PERMISSIVE);
-}
+	u8 hop_ptr, hop_cnt;
 
-/*
- * Adjust information for a received SMP
- * Return IB_SMI_DISCARD if the SMP should be dropped
- */
-enum smi_action opa_smi_handle_dr_smp_recv(struct opa_smp *smp, bool is_switch,
-					   int port_num, int phys_port_cnt)
-{
-	return __smi_handle_dr_smp_recv(is_switch, port_num, phys_port_cnt,
-					&smp->hop_ptr, smp->hop_cnt,
-					smp->route.dr.initial_path,
-					smp->route.dr.return_path,
-					opa_get_smp_direction(smp),
-					smp->route.dr.dr_dlid ==
-					OPA_LID_PERMISSIVE,
-					smp->route.dr.dr_slid ==
-					OPA_LID_PERMISSIVE);
-}
+	hop_ptr = smp->hop_ptr;
+	hop_cnt = smp->hop_cnt;
 
-static enum smi_forward_action __smi_check_forward_dr_smp(u8 hop_ptr, u8 hop_cnt,
-							  u8 direction,
-							  bool dr_dlid_is_permissive,
-							  bool dr_slid_is_permissive)
-{
-	if (!direction) {
+	if (!ib_get_smp_direction(smp)) {
 		/* C14-9:2 -- intermediate hop */
 		if (hop_ptr && hop_ptr < hop_cnt)
 			return IB_SMI_FORWARD;
 
 		/* C14-9:3 -- at the end of the DR segment of path */
 		if (hop_ptr == hop_cnt)
-			return (dr_dlid_is_permissive ?
+			return (smp->dr_dlid == IB_LID_PERMISSIVE ?
 				IB_SMI_SEND : IB_SMI_LOCAL);
 
 		/* C14-9:4 -- hop_ptr = hop_cnt + 1 -> give to SMA/SM */
@@ -292,29 +236,10 @@
 
 		/* C14-13:3 -- at the end of the DR segment of path */
 		if (hop_ptr == 1)
-			return (!dr_slid_is_permissive ?
+			return (smp->dr_slid != IB_LID_PERMISSIVE ?
 				IB_SMI_SEND : IB_SMI_LOCAL);
 	}
 	return IB_SMI_LOCAL;
-
-}
-
-enum smi_forward_action smi_check_forward_dr_smp(struct ib_smp *smp)
-{
-	return __smi_check_forward_dr_smp(smp->hop_ptr, smp->hop_cnt,
-					  ib_get_smp_direction(smp),
-					  smp->dr_dlid == IB_LID_PERMISSIVE,
-					  smp->dr_slid == IB_LID_PERMISSIVE);
-}
-
-enum smi_forward_action opa_smi_check_forward_dr_smp(struct opa_smp *smp)
-{
-	return __smi_check_forward_dr_smp(smp->hop_ptr, smp->hop_cnt,
-					  opa_get_smp_direction(smp),
-					  smp->route.dr.dr_dlid ==
-					  OPA_LID_PERMISSIVE,
-					  smp->route.dr.dr_slid ==
-					  OPA_LID_PERMISSIVE);
 }
 
 /*
@@ -326,13 +251,3 @@
 	return (!ib_get_smp_direction(smp) ? smp->initial_path[smp->hop_ptr+1] :
 		smp->return_path[smp->hop_ptr-1]);
 }
-
-/*
- * Return the forwarding port number from initial_path for outgoing SMP and
- * from return_path for returning SMP
- */
-int opa_smi_get_fwd_port(struct opa_smp *smp)
-{
-	return !opa_get_smp_direction(smp) ? smp->route.dr.initial_path[smp->hop_ptr+1] :
-		smp->route.dr.return_path[smp->hop_ptr-1];
-}
--- a/ib_mad/smi.h
+++ b/ib_mad/smi.h
@@ -51,17 +51,20 @@
 	IB_SMI_FORWARD	/* SMP should be forwarded (for switches only) */
 };
 
-enum smi_action smi_handle_dr_smp_recv(struct ib_smp *smp, bool is_switch,
+enum smi_action smi_handle_dr_smp_recv(struct ib_smp *smp, u8 node_type,
 				       int port_num, int phys_port_cnt);
 int smi_get_fwd_port(struct ib_smp *smp);
 extern enum smi_forward_action smi_check_forward_dr_smp(struct ib_smp *smp);
 extern enum smi_action smi_handle_dr_smp_send(struct ib_smp *smp,
-					      bool is_switch, int port_num);
+					      u8 node_type, int port_num);
 
 /*
  * Return IB_SMI_HANDLE if the SMP should be handled by the local SMA/SM
  * via process_mad
  */
+/* NOTE: This is called on stl_smp's don't check fields which are not common
+ * between ib_smp and stl_smp
+ */
 static inline enum smi_action smi_check_local_smp(struct ib_smp *smp,
 						  struct ib_device *device)
 {
@@ -77,6 +80,9 @@
  * Return IB_SMI_HANDLE if the SMP should be handled by the local SMA/SM
  * via process_mad
  */
+/* NOTE: This is called on stl_smp's don't check fields which are not common
+ * between ib_smp and stl_smp
+ */
 static inline enum smi_action smi_check_local_returning_smp(struct ib_smp *smp,
 						   struct ib_device *device)
 {
